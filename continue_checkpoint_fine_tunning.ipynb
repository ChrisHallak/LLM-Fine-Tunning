{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisHallak/LLM-Fine-Tunning/blob/main/continue_checkpoint_fine_tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-2_lvXlp6Lv",
        "outputId": "ef1a4531-d79c-4e49-83da-d219c7f8e1e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIKCOEUxqRul",
        "outputId": "4c572463-1065-435b-e887-4bfbf3822dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/433.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.6/460.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.46.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU datasets==3.2.0 optimum==1.24.0\n",
        "!pip install -qU openai==1.61.0 wandb\n",
        "!pip install -qU json-repair==0.29.1\n",
        "!pip install PyPDF2\n",
        "!pip install transformers -U\n",
        "!pip install bitsandbytes -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzcijjpmqcS8",
        "outputId": "5d6039fa-596c-4e5e-851d-8ccb10c512a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristeenhallak33\u001b[0m (\u001b[33mchristeenhallak33-coretech-mena\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `huggingfaceToken` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `huggingfaceToken`\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "huggingface_key = userdata.get('huggingface')\n",
        "wandb_key = userdata.get('wandb')\n",
        "openai_key = userdata.get('openai')\n",
        "\n",
        "wandb.login(key=wandb_key)\n",
        "!huggingface-cli login --token {huggingface_key}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3Ht05AfqcVG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "\n",
        "import json_repair\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "data_dir = '/gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/question_dataset_1000.jsonl'\n",
        "saved_dir = '/gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning'\n",
        "gbase_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "device = \"cuda\"\n",
        "torch_dtype = None\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DpXk8zmqcXX",
        "outputId": "8bb22ec6-59a4-4e56-a79a-c53a49a5db45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 24550, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 24550 (delta 118), reused 54 (delta 53), pack-reused 24404 (from 3)\u001b[K\n",
            "Receiving objects: 100% (24550/24550), 53.33 MiB | 13.26 MiB/s, done.\n",
            "Resolving deltas: 100% (17709/17709), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone \"https://github.com/hiyouga/LLaMA-Factory.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "TGyFEtB3qcZu",
        "outputId": "c91f6282-2eb8-400a-dc1d-e334cac99534"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n        \"news_finetune_train\": {\\n            \"file_name\": \"/gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/datasets/llamafactory-finetune-data/train.json\",\\n            \"columns\": {\\n                \"prompt\": \"instruction\",\\n                \"query\": \"input\",\\n                \"response\": \"output\",\\n                \"system\": \"system\",\\n                \"history\": \"history\"\\n            }\\n        },\\n        \"news_finetune_val\": {\\n            \"file_name\": \"/gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/datasets/llamafactory-finetune-data/val.json\",\\n            \"columns\": {\\n                \"prompt\": \"instruction\",\\n                \"query\": \"input\",\\n                \"response\": \"output\",\\n                \"system\": \"system\",\\n                \"history\": \"history\"\\n            }\\n        }\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Configure LLaMA-Factory for the new datasets\n",
        "\n",
        "# # update /content/LLaMA-Factory/data/dataset_info.json and append\n",
        "'''\n",
        "        \"news_finetune_train\": {\n",
        "            \"file_name\": \"/gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/datasets/llamafactory-finetune-data/train.json\",\n",
        "            \"columns\": {\n",
        "                \"prompt\": \"instruction\",\n",
        "                \"query\": \"input\",\n",
        "                \"response\": \"output\",\n",
        "                \"system\": \"system\",\n",
        "                \"history\": \"history\"\n",
        "            }\n",
        "        },\n",
        "        \"news_finetune_val\": {\n",
        "            \"file_name\": \"/gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/datasets/llamafactory-finetune-data/val.json\",\n",
        "            \"columns\": {\n",
        "                \"prompt\": \"instruction\",\n",
        "                \"query\": \"input\",\n",
        "                \"response\": \"output\",\n",
        "                \"system\": \"system\",\n",
        "                \"history\": \"history\"\n",
        "            }\n",
        "        }\n",
        "'''\n",
        "\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/apwbkni9\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/c5tf0q90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNjX_tv9qcbx",
        "outputId": "1668c66e-5146-4ef6-e49b-531a4f2921ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: news_finetune_train\n",
        "eval_dataset: news_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "# max_samples: 50\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "resume_from_checkpoint: /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-550\n",
        "output_dir: /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/\n",
        "logging_steps: 10\n",
        "save_strategy: steps\n",
        "save_steps: 50\n",
        "plot_loss: true\n",
        "overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 50\n",
        "\n",
        "report_to: wandb\n",
        "run_name: questions_generations_Qwen\n",
        "\n",
        "push_to_hub: true\n",
        "export_hub_model_id: \"Christeen33/questions_generations_Qwen\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in3dH26NqcfO",
        "outputId": "820dde98-eac0-483b-8845-648ee0413a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.52.0,<=4.55.0,>=4.49.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (4.55.0)\n",
            "Requirement already satisfied: datasets<=3.6.0,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (3.2.0)\n",
            "Collecting accelerate<=1.7.0,>=1.3.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft<=0.15.2,>=0.14.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tokenizers<=0.21.1,>=0.19.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting gradio<=5.31.0,>=4.38.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (3.10.0)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.8.1)\n",
            "Collecting numpy<2.0.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (1.16.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.10.0)\n",
            "Collecting modelscope>=1.14.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading modelscope-1.28.2-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.1.9)\n",
            "Collecting safetensors<=0.5.3 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting fire (from llamafactory==0.9.4.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (6.0.2)\n",
            "Collecting pydantic<=2.10.6 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.35.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.116.1)\n",
            "Collecting sse-starlette (from llamafactory==0.9.4.dev0)\n",
            "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting av (from llamafactory==0.9.4.dev0)\n",
            "  Downloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.11.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.34.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.12.15)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.10.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.6.1)\n",
            "Collecting gradio-client==1.10.1 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11.1)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.12.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.14.1)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (2.5.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.4.dev0)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,<=4.55.0,>=4.49.0->llamafactory==0.9.4.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.4.dev0) (3.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.1.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->llamafactory==0.9.4.dev0) (4.9.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.20.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.1.7)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.4.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.4.dev0) (4.3.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.28.2-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.4.dev0-0.editable-py3-none-any.whl size=27876 sha256=716c089edc68dad4fef575104451385f1cc521439076341742bb92078712f939\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-77kx8jme/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=234c005894df970bdbaac92753a41b1779eb9a5a9929a9d75377b56fb4dc8ca2\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: shtab, safetensors, pydantic-core, numpy, fire, av, sse-starlette, pydantic, modelscope, tyro, tokenizers, gradio-client, gradio, accelerate, trl, peft, llamafactory\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.6.1\n",
            "    Uninstalling safetensors-0.6.1:\n",
            "      Successfully uninstalled safetensors-0.6.1\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.4\n",
            "    Uninstalling tokenizers-0.21.4:\n",
            "      Successfully uninstalled tokenizers-0.21.4\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.11.0\n",
            "    Uninstalling gradio_client-1.11.0:\n",
            "      Successfully uninstalled gradio_client-1.11.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.41.0\n",
            "    Uninstalling gradio-5.41.0:\n",
            "      Successfully uninstalled gradio-5.41.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.9.0\n",
            "    Uninstalling accelerate-1.9.0:\n",
            "      Successfully uninstalled accelerate-1.9.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.0\n",
            "    Uninstalling peft-0.17.0:\n",
            "      Successfully uninstalled peft-0.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.7.0 av-15.0.0 fire-0.7.0 gradio-5.31.0 gradio-client-1.10.1 llamafactory-0.9.4.dev0 modelscope-1.28.2 numpy-1.26.4 peft-0.15.2 pydantic-2.10.6 pydantic-core-2.27.2 safetensors-0.5.3 shtab-1.7.2 sse-starlette-3.0.2 tokenizers-0.21.1 trl-0.9.6 tyro-0.8.14\n"
          ]
        }
      ],
      "source": [
        "# # STEP 1: Clone the repo\n",
        "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "\n",
        "# STEP 2: Change directory\n",
        "%cd LLaMA-Factory\n",
        "\n",
        "# STEP 3: Install in editable mode\n",
        "!pip install -e .\n",
        "\n",
        "# Force restart the runtime (only works in Colab)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # This will crash the runtime and force a restart\n",
        "\n",
        "# After restarting, run only the training command in a new cell:\n",
        "# !llamafactory-cli train examples/train_lora/news_finetune.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU50lv0NrDnd",
        "outputId": "082c52aa-d539-4d5e-b6e4-50ff76842b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrnJ8iQIrDpu",
        "outputId": "e2190d74-11d0-4316-d632-f02110b8328d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-10 07:07:45.091879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754809665.354789    4404 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754809665.429266    4404 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754809665.985699    4404 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754809665.985743    4404 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754809665.985747    4404 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754809665.985753    4404 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-10 07:07:46.036167: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-08-10 07:08:00] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "tokenizer_config.json: 7.30kB [00:00, 26.5MB/s]\n",
            "vocab.json: 2.78MB [00:00, 11.4MB/s]\n",
            "merges.txt: 1.67MB [00:00, 9.96MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 23.7MB/s]\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:03,147 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:03,147 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:03,147 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:03,147 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:03,147 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:03,147 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:03,147 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2336] 2025-08-10 07:08:03,510 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100% 660/660 [00:00<00:00, 3.89MB/s]\n",
            "[INFO|configuration_utils.py:752] 2025-08-10 07:08:04,254 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-10 07:08:04,264 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:04,468 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:04,468 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:04,468 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:04,468 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:04,468 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:04,468 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2067] 2025-08-10 07:08:04,468 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2336] 2025-08-10 07:08:04,815 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-08-10 07:08:04] llamafactory.data.loader:143 >> Loading dataset /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/datasets/llamafactory-finetune-data/train.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "WARNING:datasets.builder:Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 1000 examples [00:00, 1557.86 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 1000/1000 [00:00<00:00, 1240.98 examples/s]\n",
            "[INFO|2025-08-10 07:08:09] llamafactory.data.loader:143 >> Loading dataset /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/datasets/llamafactory-finetune-data/val.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "WARNING:datasets.builder:Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 175 examples [00:00, 628.43 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 175/175 [00:00<00:00, 377.49 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:22<00:00, 45.08 examples/s] \n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 3405, 9471, 6203, 198, 2610, 686, 387, 3897, 553, 264, 11358, 2213, 5815, 448, 264, 5355, 67, 8159, 12859, 38088, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 323, 279, 1565, 5097, 43781, 63, 311, 6923, 279, 1565, 5097, 4718, 18639, 5404, 537, 6923, 894, 16800, 476, 16688, 13, 151645, 198, 151644, 872, 198, 2, 11358, 35768, 510, 334, 142630, 25, 133453, 73441, 130918, 125013, 77273, 131359, 128438, 73441, 56177, 124464, 14293, 83827, 130918, 125013, 63237, 123894, 138201, 139744, 128261, 39434, 130135, 129366, 127837, 68238, 14558, 124623, 127837, 77273, 139542, 124104, 128438, 73441, 68785, 128476, 50243, 125510, 130543, 39434, 124035, 124661, 77273, 127519, 82168, 126277, 21360, 139542, 124104, 13, 130044, 125861, 125013, 134436, 142001, 63415, 128801, 49388, 132294, 65398, 130434, 68785, 126420, 128420, 124665, 123832, 124061, 125006, 140851, 37524, 124552, 143331, 47632, 13, 4710, 23224, 124523, 124009, 50243, 127038, 125006, 125290, 124085, 68785, 50243, 129425, 130918, 125013, 56794, 130099, 94957, 124130, 126413, 124058, 124184, 125756, 125006, 123993, 23224, 37524, 139285, 124126, 124006, 124476, 129725, 126655, 128261, 50243, 124062, 91335, 5703, 126554, 124434, 124104, 13, 126350, 85153, 127329, 140591, 68785, 50243, 125856, 27846, 136701, 53479, 124655, 47632, 138829, 124061, 56794, 57859, 125559, 50243, 33090, 124210, 124666, 125657, 25871, 13, 128641, 53479, 124144, 124966, 68785, 39434, 129520, 124104, 130918, 125013, 77273, 39434, 43635, 127110, 23364, 16157, 124669, 94957, 124552, 95198, 124072, 140851, 132474, 79820, 68785, 128660, 130656, 126815, 11071, 125729, 77273, 127519, 94957, 127564, 47632, 123877, 31073, 65398, 124176, 73441, 382, 91962, 57859, 125185, 124376, 128252, 128349, 68785, 39434, 130135, 130918, 125013, 129366, 127837, 141182, 127837, 77273, 123894, 138201, 137869, 73441, 128718, 124114, 14558, 125089, 98719, 124072, 16157, 124523, 124966, 13, 45577, 131444, 124172, 126277, 123860, 130918, 73441, 73274, 129520, 140123, 128248, 130546, 10176, 124176, 125108, 130434, 124821, 124075, 124130, 126196, 124042, 25871, 13, 128388, 39434, 129425, 130918, 125013, 77273, 131132, 47632, 94957, 124218, 127808, 124075, 128405, 127343, 68785, 128476, 39434, 135531, 77273, 39434, 43635, 127110, 125332, 127641, 125013, 37524, 135188, 138750, 382, 14293, 135022, 133453, 73441, 130918, 125013, 130740, 69423, 17166, 131412, 68785, 45577, 132554, 130918, 125013, 73274, 129520, 77273, 126731, 125089, 39697, 124058, 124187, 123862, 124072, 70604, 125492, 35038, 13, 137687, 39434, 123987, 73771, 123904, 5703, 127046, 50243, 124552, 143331, 47632, 27846, 130763, 63237, 127839, 73441, 68785, 137050, 125007, 128731, 129046, 39434, 69682, 125194, 47632, 85153, 125677, 70604, 73441, 77273, 139542, 124104, 142822, 126687, 124072, 142538, 73441, 382, 8532, 124169, 68785, 63237, 17166, 125248, 125729, 125007, 50243, 129657, 130918, 125013, 17166, 134612, 17166, 140774, 77273, 53479, 125573, 20064, 68785, 37524, 124983, 33090, 23224, 140123, 128248, 93153, 31073, 32790, 95975, 82168, 125291, 128280, 129726, 37524, 125830, 123938, 126808, 16157, 128631, 124079, 124267, 13, 63237, 128374, 45577, 124138, 130918, 125013, 68785, 126298, 125006, 124525, 70604, 125007, 73274, 126933, 123920, 126492, 124388, 123860, 37524, 132265, 127116, 128248, 23364, 131377, 25871, 94957, 29825, 79820, 47632, 128261, 128416, 39434, 131377, 124138, 77273, 138471, 624, 2, 5430, 510, 31115, 220, 16, 15, 34117, 5248, 62626, 4755, 504, 279, 2701, 16229, 21085, 13, 53591, 1070, 1105, 553, 16829, 11, 24503, 748, 71806, 11, 323, 2033, 882, 624, 2, 9258, 43781, 510, 4913, 3, 48485, 788, 5212, 14582, 788, 5212, 13193, 788, 5212, 7841, 4326, 788, 5212, 4684, 788, 330, 14582, 1467, 10465, 330, 2102, 788, 330, 14582, 2918, 497, 330, 1313, 788, 330, 917, 14345, 330, 7841, 61610, 788, 5212, 4684, 788, 330, 852, 315, 4226, 2606, 10465, 330, 3615, 788, 5212, 1313, 788, 330, 917, 14345, 330, 1065, 4353, 788, 220, 17, 11, 330, 2102, 788, 330, 14582, 37243, 497, 330, 1313, 788, 330, 1653, 14345, 330, 19928, 28534, 788, 5212, 4684, 788, 330, 785, 4396, 4226, 10465, 330, 2102, 788, 330, 33092, 21806, 497, 330, 1313, 788, 330, 917, 14345, 330, 13193, 788, 5212, 3, 1097, 788, 5869, 10749, 48485, 14, 14582, 7903, 497, 330, 4684, 788, 330, 3889, 66415, 8201, 1189, 38154, 330, 6279, 788, 4383, 7841, 4326, 497, 330, 7841, 61610, 497, 330, 19928, 28534, 497, 330, 13193, 7914, 330, 2102, 788, 330, 14582, 497, 330, 1313, 788, 330, 1700, 14345, 330, 14582, 7903, 788, 5212, 13193, 788, 5212, 72109, 788, 5212, 9018, 788, 4383, 25756, 18609, 497, 330, 36730, 497, 330, 26292, 497, 330, 21751, 3866, 7914, 330, 2102, 788, 330, 72109, 497, 330, 1313, 788, 330, 917, 9207, 2137, 330, 6279, 788, 4383, 72109, 7914, 330, 2102, 788, 330, 14582, 7903, 497, 330, 1313, 788, 330, 1700, 9207, 2137, 330, 13193, 788, 5212, 17348, 788, 5212, 4684, 788, 330, 852, 315, 20869, 48, 82, 497, 330, 3615, 788, 5212, 3, 1097, 788, 5869, 10749, 48485, 14, 14582, 14345, 330, 2102, 788, 330, 35249, 497, 330, 1313, 788, 330, 1653, 9207, 2137, 330, 6279, 788, 4383, 17348, 7914, 330, 2102, 788, 330, 14582, 1649, 497, 330, 1313, 788, 330, 1700, 63159, 2, 9258, 4718, 510, 73594, 2236, 151645, 198, 151644, 77091, 198, 73594, 2236, 198, 14129, 59, 77, 262, 7245, 17348, 11693, 93815, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 129366, 130918, 125013, 77273, 94957, 132111, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 130099, 94957, 124130, 126413, 124058, 124184, 125756, 65290, 77, 394, 7245, 131808, 131876, 123894, 8532, 125192, 136411, 65290, 77, 394, 7245, 125693, 125633, 128546, 133663, 126196, 131101, 65290, 77, 394, 7245, 131808, 128912, 53479, 125290, 124085, 123860, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 130099, 94957, 124130, 126413, 124058, 124184, 125756, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 128538, 137868, 123938, 126808, 128438, 73441, 125006, 125861, 125013, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 136701, 53479, 124655, 47632, 65290, 77, 394, 7245, 136388, 136875, 65290, 77, 394, 7245, 136664, 37524, 133617, 135203, 134934, 65290, 77, 394, 7245, 31382, 132451, 25871, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 136701, 53479, 124655, 47632, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 39434, 129520, 130918, 125013, 77273, 39434, 43635, 127110, 23364, 16157, 124669, 140123, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 131578, 124138, 127046, 73274, 124042, 132280, 129816, 65290, 77, 394, 7245, 137536, 23364, 16157, 124669, 94957, 124552, 95198, 124072, 140851, 132474, 79820, 65290, 77, 394, 7245, 124706, 127226, 23364, 16157, 124669, 134145, 25871, 65290, 77, 394, 7245, 125089, 124346, 77703, 124144, 129080, 128248, 17166, 143800, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 137536, 23364, 16157, 124669, 94957, 124552, 95198, 124072, 140851, 132474, 79820, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 39434, 69682, 125194, 39434, 129484, 130918, 125013, 128248, 17166, 70604, 125492, 35038, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 80970, 140620, 124661, 65290, 77, 394, 7245, 124280, 39697, 39697, 124058, 124187, 123862, 128835, 124653, 94957, 133157, 93543, 132474, 79820, 65290, 77, 394, 7245, 14558, 124388, 8532, 63237, 124058, 124187, 123862, 65290, 77, 394, 7245, 14558, 124035, 79820, 128252, 93153, 124876, 65398, 123877, 20931, 126900, 132061, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 124280, 39697, 39697, 124058, 124187, 123862, 128835, 124653, 94957, 133157, 93543, 132474, 79820, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 39434, 129425, 130918, 125013, 77273, 123894, 138201, 137869, 73441, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 8532, 131808, 123894, 123920, 125346, 65290, 77, 394, 7245, 8532, 134235, 125108, 130434, 124821, 124075, 124130, 126196, 124042, 25871, 65290, 77, 394, 7245, 8532, 130221, 132733, 65290, 77, 394, 7245, 8532, 131403, 142822, 124669, 136355, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 8532, 134235, 125108, 130434, 124821, 124075, 124130, 126196, 124042, 25871, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28497, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 8532, 133632, 129539, 128248, 53479, 125573, 20064, 17166, 134612, 124476, 125861, 125013, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 8532, 124229, 16157, 130226, 23364, 124014, 65290, 77, 394, 7245, 8532, 124229, 16157, 73274, 129520, 77273, 94957, 129484, 125616, 124388, 37524, 133719, 94957, 29825, 79820, 47632, 65290, 77, 394, 7245, 8532, 124229, 16157, 129581, 125490, 39434, 69682, 125194, 128248, 140123, 65290, 77, 394, 7245, 8532, 124229, 16157, 135048, 128248, 126731, 125089, 39697, 17166, 143800, 129346, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 8532, 124229, 16157, 73274, 129520, 77273, 94957, 129484, 125616, 124388, 37524, 133719, 94957, 29825, 79820, 47632, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 123894, 132027, 139089, 125309, 125006, 125861, 125013, 37524, 129152, 127837, 125006, 126723, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 10176, 124328, 13325, 63415, 128801, 49388, 132294, 65398, 130434, 65290, 77, 394, 7245, 43635, 123832, 124061, 125006, 140851, 37524, 124552, 143331, 47632, 65290, 77, 394, 7245, 140713, 39434, 133420, 129346, 77273, 53479, 125573, 20064, 65290, 77, 394, 7245, 125463, 129536, 56794, 136260, 124265, 58656, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 43635, 123832, 124061, 125006, 140851, 37524, 124552, 143331, 47632, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 73274, 135531, 45577, 124138, 124172, 126277, 123860, 130918, 73441, 77273, 123894, 138201, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 124392, 125750, 45577, 124138, 77703, 12653, 126606, 137057, 65290, 77, 394, 7245, 124392, 126606, 77273, 130546, 10176, 124176, 123877, 130018, 124325, 129200, 124042, 25871, 65290, 77, 394, 7245, 14558, 64604, 29825, 41593, 52704, 73771, 8532, 128248, 76841, 33090, 47632, 137992, 65290, 77, 394, 7245, 80970, 140620, 124661, 128248, 124080, 124787, 125168, 124080, 16157, 126510, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 124392, 126606, 77273, 130546, 10176, 124176, 123877, 130018, 124325, 129200, 124042, 25871, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 140691, 130918, 125013, 132548, 124218, 127808, 124075, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 125290, 5703, 124138, 77273, 39434, 43635, 127110, 125332, 127641, 125013, 37524, 135188, 138750, 65290, 77, 394, 7245, 14293, 129425, 129346, 77273, 123877, 130573, 70604, 65290, 77, 394, 7245, 127300, 129046, 124793, 140691, 65290, 77, 394, 7245, 124706, 125362, 125503, 124058, 125573, 14558, 129346, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 125290, 5703, 124138, 77273, 39434, 43635, 127110, 125332, 127641, 125013, 37524, 135188, 138750, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 133453, 73441, 130918, 125013, 77273, 131359, 139477, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 123997, 8532, 128858, 125358, 5703, 124130, 65290, 77, 394, 7245, 132554, 124665, 128801, 63237, 127839, 73441, 56794, 124552, 143331, 47632, 65290, 77, 394, 7245, 124425, 123987, 131359, 128962, 125750, 65290, 77, 394, 7245, 80970, 39434, 127915, 124476, 133239, 139477, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 132554, 124665, 128801, 63237, 127839, 73441, 56794, 124552, 143331, 47632, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28497, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 51300, 77, 262, 2279, 59, 77, 11195, 73594, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a question generation expert\n",
            "You will be provided by a PDF content associated with a Pydantic scheme.,\n",
            "Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\n",
            "Do not generate any introduction or conclusion.<|im_end|>\n",
            "<|im_start|>user\n",
            "# PDF CONTENT:\n",
            "**موضوع: أهمية الرياضيات في الحياة اليومية**\n",
            "\n",
            "تعتبر الرياضيات من العلوم الأساسية التي تلعب دورًا حيويًا في حياتنا اليومية، حيث نجد أنها تؤثر في مختلف جوانب حياتنا. فالرياضيات ليست مجرد أرقام ومعادلات، بل هي طريقة للتفكير وحل المشكلات. \n",
            "\n",
            "عندما نذهب للتسوق، نستخدم الرياضيات لحساب التكلفة الإجمالية للسلع ومقارنتها بالميزانية التي نحددها لأنفسنا. عند إعداد الطعام، نقوم بقياس المكونات بدقة لضمان نجاح الوصفة. وفي المدرسة، تساعدنا الرياضيات في تطوير مهارات التحليل والتفكير النقدي، وهو أمر ضروري في مختلف التخصصات الأكاديمية.\n",
            "\n",
            "إضافةً إلى ذلك، تلعب الرياضيات دورًا مهمًا في العلوم التطبيقية مثل الفيزياء والهندسة. ففهم القوانين الرياضية يساعد الطلاب على تصميم آلات وهياكل معقدة. كما تستخدم الرياضيات في مجالات التكنولوجيا والمعلومات، حيث تساهم في تطوير البرمجيات وتحليل البيانات.\n",
            "\n",
            "تتجاوز أهمية الرياضيات حدود الدراسة، فتعلم الرياضيات يساعد في تعزيز الإبداع والابتكار. فهي تعلّمنا كيف نحل المشكلات بطرق منهجية، ويمكن أن يكون لها تأثيرات إيجابية في حياتنا المهنية والشخصية.\n",
            "\n",
            "لذا، من الضروري أن نولي الرياضيات الاهتمام اللازم في المدارس، ونشجع الطلاب على استكشاف جمال هذا العلم وتطبيقاته المتعددة. من خلال فهم الرياضيات، يمكن للطلاب أن يصبحوا مستقلين وقادرين على مواجهة التحديات التي قد تواجههم في المستقبل.\n",
            "# Task:\n",
            "Generate 10 Arabic multiple-choice questions from the following educational passage. Distribute them by difficulty, Bloom’s taxonomy, and response time.\n",
            "# Output Scheme:\n",
            "{\"$defs\": {\"Question\": {\"properties\": {\"question_text\": {\"description\": \"Question text.\", \"title\": \"Question Text\", \"type\": \"string\"}, \"question_answers\": {\"description\": \"List of answer options.\", \"items\": {\"type\": \"string\"}, \"minItems\": 2, \"title\": \"Question Answers\", \"type\": \"array\"}, \"correct_answer\": {\"description\": \"The correct answer.\", \"title\": \"Correct Answer\", \"type\": \"string\"}, \"properties\": {\"$ref\": \"#/$defs/QuestionProperties\", \"description\": \"Per-question attributes.\"}}, \"required\": [\"question_text\", \"question_answers\", \"correct_answer\", \"properties\"], \"title\": \"Question\", \"type\": \"object\"}, \"QuestionProperties\": {\"properties\": {\"Difficulty\": {\"enum\": [\"Very Easy\", \"Easy\", \"Average\", \"Difficult\"], \"title\": \"Difficulty\", \"type\": \"string\"}}, \"required\": [\"Difficulty\"], \"title\": \"QuestionProperties\", \"type\": \"object\"}}, \"properties\": {\"questions\": {\"description\": \"List of MCQs\", \"items\": {\"$ref\": \"#/$defs/Question\"}, \"title\": \"Questions\", \"type\": \"array\"}}, \"required\": [\"questions\"], \"title\": \"QuestionSet\", \"type\": \"object\"}\n",
            "\n",
            "# Output JSON:\n",
            "```json<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "\"{\\n    \\\"questions\\\": [\\n        {\\n            \\\"question_text\\\": \\\"ما هو دور الرياضيات في التسوق؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"حساب التكلفة الإجمالية\\\",\\n                \\\"تحديد نوع العلامة التجارية\\\",\\n                \\\"اختيار متاجر معينة\\\",\\n                \\\"تحديد عدد المتسوقين\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"حساب التكلفة الإجمالية\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هي بعض التطبيقات اليومية للرياضيات؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"قياس المكونات\\\",\\n                \\\"تنظيف المنزل\\\",\\n                \\\"متابعة وسائل التواصل الاجتماعي\\\",\\n                \\\"القراءة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"قياس المكونات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف تساعد الرياضيات في تطوير مهارات الطلاب؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تعليمهم كيف يقدرون الوقت\\\",\\n                \\\"تحسين مهارات التحليل والتفكير النقدي\\\",\\n                \\\"تنمية مهارات الكتابة\\\",\\n                \\\"زيادة قدرتهم على الذاكرة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تحسين مهارات التحليل والتفكير النقدي\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو تأثير تعلم الرياضيات على الابتكار؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"لا يؤثر\\\",\\n                \\\"يعزز الإبداع وقدرة التفكير النقدي\\\",\\n                \\\"يقلل من الإبداع\\\",\\n                \\\"يؤدي إلى استبعاد الأفكار الجديدة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"يعزز الإبداع وقدرة التفكير النقدي\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف تستخدم الرياضيات في العلوم التطبيقية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"لتحديد العواطف\\\",\\n                \\\"لتصميم آلات وهياكل معقدة\\\",\\n                \\\"لتنظيم المعلومات\\\",\\n                \\\"لتطوير المهارات الاجتماعية\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"لتصميم آلات وهياكل معقدة\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Apply\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"لماذا يجب على المدارس الاهتمام بالرياضيات؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"لأنه علم ممل\\\",\\n                \\\"لأنه يساعد في التعلم المستقل ومواجهة التحديات\\\",\\n                \\\"لأنه ليس له تأثير على الطلاب\\\",\\n                \\\"لأنه يعمل على تعزيز الذاكرة فقط\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"لأنه يساعد في التعلم المستقل ومواجهة التحديات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو العنصر الأساسي للرياضيات وفقًا للنص؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"مجرد أرقام ومعادلات\\\",\\n                \\\"طريقة للتفكير وحل المشكلات\\\",\\n                \\\"مادة تدرس فقط في المدارس\\\",\\n                \\\"وسيلة لرسم الصور\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"طريقة للتفكير وحل المشكلات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف يساهم فهم القوانين الرياضية في العلوم؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"يسهل فهم قواعد اللغة\\\",\\n                \\\"يساعد في تصميم الأنظمة المعقدة\\\",\\n                \\\"يُحصِّل على درجات أعلى\\\",\\n                \\\"لا يؤثر على النتيجة النهائية\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"يساعد في تصميم الأنظمة المعقدة\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما علاقة الرياضيات بالتكنولوجيا؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تساهم في تطوير البرمجيات وتحليل البيانات\\\",\\n                \\\"تستخدم فقط في الألعاب\\\",\\n                \\\"ليس لها أي علاقة\\\",\\n                \\\"تنظم العمل الإداري فقط\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تساهم في تطوير البرمجيات وتحليل البيانات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هي أهمية الرياضيات في الحياة الشخصية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تحل جميع المشاكل\\\",\\n                \\\"تعلم طرق منهجية لحل المشكلات\\\",\\n                \\\"تجعل الحياة أسهل\\\",\\n                \\\"لا تتعلق بالحياة الشخصية\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تعلم طرق منهجية لحل المشكلات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Apply\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        }\\n    ]\\n}\"\n",
            "```<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 14129, 59, 77, 262, 7245, 17348, 11693, 93815, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 129366, 130918, 125013, 77273, 94957, 132111, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 130099, 94957, 124130, 126413, 124058, 124184, 125756, 65290, 77, 394, 7245, 131808, 131876, 123894, 8532, 125192, 136411, 65290, 77, 394, 7245, 125693, 125633, 128546, 133663, 126196, 131101, 65290, 77, 394, 7245, 131808, 128912, 53479, 125290, 124085, 123860, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 130099, 94957, 124130, 126413, 124058, 124184, 125756, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 128538, 137868, 123938, 126808, 128438, 73441, 125006, 125861, 125013, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 136701, 53479, 124655, 47632, 65290, 77, 394, 7245, 136388, 136875, 65290, 77, 394, 7245, 136664, 37524, 133617, 135203, 134934, 65290, 77, 394, 7245, 31382, 132451, 25871, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 136701, 53479, 124655, 47632, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 39434, 129520, 130918, 125013, 77273, 39434, 43635, 127110, 23364, 16157, 124669, 140123, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 131578, 124138, 127046, 73274, 124042, 132280, 129816, 65290, 77, 394, 7245, 137536, 23364, 16157, 124669, 94957, 124552, 95198, 124072, 140851, 132474, 79820, 65290, 77, 394, 7245, 124706, 127226, 23364, 16157, 124669, 134145, 25871, 65290, 77, 394, 7245, 125089, 124346, 77703, 124144, 129080, 128248, 17166, 143800, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 137536, 23364, 16157, 124669, 94957, 124552, 95198, 124072, 140851, 132474, 79820, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 39434, 69682, 125194, 39434, 129484, 130918, 125013, 128248, 17166, 70604, 125492, 35038, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 80970, 140620, 124661, 65290, 77, 394, 7245, 124280, 39697, 39697, 124058, 124187, 123862, 128835, 124653, 94957, 133157, 93543, 132474, 79820, 65290, 77, 394, 7245, 14558, 124388, 8532, 63237, 124058, 124187, 123862, 65290, 77, 394, 7245, 14558, 124035, 79820, 128252, 93153, 124876, 65398, 123877, 20931, 126900, 132061, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 124280, 39697, 39697, 124058, 124187, 123862, 128835, 124653, 94957, 133157, 93543, 132474, 79820, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 39434, 129425, 130918, 125013, 77273, 123894, 138201, 137869, 73441, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 8532, 131808, 123894, 123920, 125346, 65290, 77, 394, 7245, 8532, 134235, 125108, 130434, 124821, 124075, 124130, 126196, 124042, 25871, 65290, 77, 394, 7245, 8532, 130221, 132733, 65290, 77, 394, 7245, 8532, 131403, 142822, 124669, 136355, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 8532, 134235, 125108, 130434, 124821, 124075, 124130, 126196, 124042, 25871, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28497, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 8532, 133632, 129539, 128248, 53479, 125573, 20064, 17166, 134612, 124476, 125861, 125013, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 8532, 124229, 16157, 130226, 23364, 124014, 65290, 77, 394, 7245, 8532, 124229, 16157, 73274, 129520, 77273, 94957, 129484, 125616, 124388, 37524, 133719, 94957, 29825, 79820, 47632, 65290, 77, 394, 7245, 8532, 124229, 16157, 129581, 125490, 39434, 69682, 125194, 128248, 140123, 65290, 77, 394, 7245, 8532, 124229, 16157, 135048, 128248, 126731, 125089, 39697, 17166, 143800, 129346, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 8532, 124229, 16157, 73274, 129520, 77273, 94957, 129484, 125616, 124388, 37524, 133719, 94957, 29825, 79820, 47632, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 123894, 132027, 139089, 125309, 125006, 125861, 125013, 37524, 129152, 127837, 125006, 126723, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 10176, 124328, 13325, 63415, 128801, 49388, 132294, 65398, 130434, 65290, 77, 394, 7245, 43635, 123832, 124061, 125006, 140851, 37524, 124552, 143331, 47632, 65290, 77, 394, 7245, 140713, 39434, 133420, 129346, 77273, 53479, 125573, 20064, 65290, 77, 394, 7245, 125463, 129536, 56794, 136260, 124265, 58656, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 43635, 123832, 124061, 125006, 140851, 37524, 124552, 143331, 47632, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 73274, 135531, 45577, 124138, 124172, 126277, 123860, 130918, 73441, 77273, 123894, 138201, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 124392, 125750, 45577, 124138, 77703, 12653, 126606, 137057, 65290, 77, 394, 7245, 124392, 126606, 77273, 130546, 10176, 124176, 123877, 130018, 124325, 129200, 124042, 25871, 65290, 77, 394, 7245, 14558, 64604, 29825, 41593, 52704, 73771, 8532, 128248, 76841, 33090, 47632, 137992, 65290, 77, 394, 7245, 80970, 140620, 124661, 128248, 124080, 124787, 125168, 124080, 16157, 126510, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 124392, 126606, 77273, 130546, 10176, 124176, 123877, 130018, 124325, 129200, 124042, 25871, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 140691, 130918, 125013, 132548, 124218, 127808, 124075, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 125290, 5703, 124138, 77273, 39434, 43635, 127110, 125332, 127641, 125013, 37524, 135188, 138750, 65290, 77, 394, 7245, 14293, 129425, 129346, 77273, 123877, 130573, 70604, 65290, 77, 394, 7245, 127300, 129046, 124793, 140691, 65290, 77, 394, 7245, 124706, 125362, 125503, 124058, 125573, 14558, 129346, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 125290, 5703, 124138, 77273, 39434, 43635, 127110, 125332, 127641, 125013, 37524, 135188, 138750, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 133453, 73441, 130918, 125013, 77273, 131359, 139477, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 123997, 8532, 128858, 125358, 5703, 124130, 65290, 77, 394, 7245, 132554, 124665, 128801, 63237, 127839, 73441, 56794, 124552, 143331, 47632, 65290, 77, 394, 7245, 124425, 123987, 131359, 128962, 125750, 65290, 77, 394, 7245, 80970, 39434, 127915, 124476, 133239, 139477, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 132554, 124665, 128801, 63237, 127839, 73441, 56794, 124552, 143331, 47632, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28497, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 51300, 77, 262, 2279, 59, 77, 11195, 73594, 151645, 198]\n",
            "labels:\n",
            "```json\n",
            "\"{\\n    \\\"questions\\\": [\\n        {\\n            \\\"question_text\\\": \\\"ما هو دور الرياضيات في التسوق؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"حساب التكلفة الإجمالية\\\",\\n                \\\"تحديد نوع العلامة التجارية\\\",\\n                \\\"اختيار متاجر معينة\\\",\\n                \\\"تحديد عدد المتسوقين\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"حساب التكلفة الإجمالية\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هي بعض التطبيقات اليومية للرياضيات؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"قياس المكونات\\\",\\n                \\\"تنظيف المنزل\\\",\\n                \\\"متابعة وسائل التواصل الاجتماعي\\\",\\n                \\\"القراءة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"قياس المكونات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف تساعد الرياضيات في تطوير مهارات الطلاب؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تعليمهم كيف يقدرون الوقت\\\",\\n                \\\"تحسين مهارات التحليل والتفكير النقدي\\\",\\n                \\\"تنمية مهارات الكتابة\\\",\\n                \\\"زيادة قدرتهم على الذاكرة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تحسين مهارات التحليل والتفكير النقدي\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو تأثير تعلم الرياضيات على الابتكار؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"لا يؤثر\\\",\\n                \\\"يعزز الإبداع وقدرة التفكير النقدي\\\",\\n                \\\"يقلل من الإبداع\\\",\\n                \\\"يؤدي إلى استبعاد الأفكار الجديدة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"يعزز الإبداع وقدرة التفكير النقدي\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف تستخدم الرياضيات في العلوم التطبيقية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"لتحديد العواطف\\\",\\n                \\\"لتصميم آلات وهياكل معقدة\\\",\\n                \\\"لتنظيم المعلومات\\\",\\n                \\\"لتطوير المهارات الاجتماعية\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"لتصميم آلات وهياكل معقدة\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Apply\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"لماذا يجب على المدارس الاهتمام بالرياضيات؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"لأنه علم ممل\\\",\\n                \\\"لأنه يساعد في التعلم المستقل ومواجهة التحديات\\\",\\n                \\\"لأنه ليس له تأثير على الطلاب\\\",\\n                \\\"لأنه يعمل على تعزيز الذاكرة فقط\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"لأنه يساعد في التعلم المستقل ومواجهة التحديات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو العنصر الأساسي للرياضيات وفقًا للنص؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"مجرد أرقام ومعادلات\\\",\\n                \\\"طريقة للتفكير وحل المشكلات\\\",\\n                \\\"مادة تدرس فقط في المدارس\\\",\\n                \\\"وسيلة لرسم الصور\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"طريقة للتفكير وحل المشكلات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف يساهم فهم القوانين الرياضية في العلوم؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"يسهل فهم قواعد اللغة\\\",\\n                \\\"يساعد في تصميم الأنظمة المعقدة\\\",\\n                \\\"يُحصِّل على درجات أعلى\\\",\\n                \\\"لا يؤثر على النتيجة النهائية\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"يساعد في تصميم الأنظمة المعقدة\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما علاقة الرياضيات بالتكنولوجيا؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تساهم في تطوير البرمجيات وتحليل البيانات\\\",\\n                \\\"تستخدم فقط في الألعاب\\\",\\n                \\\"ليس لها أي علاقة\\\",\\n                \\\"تنظم العمل الإداري فقط\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تساهم في تطوير البرمجيات وتحليل البيانات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هي أهمية الرياضيات في الحياة الشخصية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تحل جميع المشاكل\\\",\\n                \\\"تعلم طرق منهجية لحل المشكلات\\\",\\n                \\\"تجعل الحياة أسهل\\\",\\n                \\\"لا تتعلق بالحياة الشخصية\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تعلم طرق منهجية لحل المشكلات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Apply\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        }\\n    ]\\n}\"\n",
            "```<|im_end|>\n",
            "\n",
            "Running tokenizer on dataset (num_proc=16): 100% 175/175 [00:11<00:00, 15.84 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 3405, 9471, 6203, 198, 2610, 686, 387, 3897, 553, 264, 11358, 2213, 5815, 448, 264, 5355, 67, 8159, 12859, 38088, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 323, 279, 1565, 5097, 43781, 63, 311, 6923, 279, 1565, 5097, 4718, 18639, 5404, 537, 6923, 894, 16800, 476, 16688, 13, 151645, 198, 151644, 872, 198, 2, 11358, 35768, 510, 334, 127206, 126277, 25, 133453, 73441, 130918, 125013, 77273, 139542, 124104, 128438, 73441, 56177, 124464, 14293, 83827, 130918, 125013, 63237, 133453, 123894, 138201, 128261, 130243, 124138, 77273, 39434, 131446, 95198, 94957, 133157, 93543, 125088, 43635, 123995, 124072, 134951, 128248, 130260, 143331, 47632, 13, 129015, 123920, 125510, 130918, 125013, 77273, 128858, 131132, 47632, 139542, 124104, 128438, 73441, 68785, 128476, 50243, 129425, 124006, 77273, 130684, 63237, 123877, 124983, 124917, 126249, 124142, 124917, 128718, 94957, 132111, 68785, 68238, 136176, 135887, 126808, 68785, 139083, 126350, 85153, 127329, 140591, 13, 45577, 23224, 124523, 124009, 50243, 127038, 128252, 123913, 130945, 23364, 35038, 124514, 68785, 50243, 126157, 128252, 68238, 136176, 94957, 124130, 126413, 124058, 124184, 125756, 129232, 125768, 95975, 128546, 124079, 124267, 68785, 130267, 128657, 128742, 23364, 16157, 124669, 53710, 124075, 57859, 73441, 136574, 382, 127472, 91962, 57859, 125185, 128252, 128349, 68785, 39434, 64604, 129425, 130918, 125013, 77273, 131132, 47632, 128773, 126731, 136503, 127837, 128718, 136157, 124966, 124072, 83827, 10176, 125168, 13, 73274, 133420, 142822, 124523, 20064, 123890, 130918, 125013, 56794, 129196, 125100, 124075, 124130, 124072, 134235, 47632, 68785, 134939, 73274, 132448, 53479, 142838, 123890, 128248, 128962, 20064, 53710, 124075, 57859, 73441, 56794, 131403, 125332, 127641, 125013, 124072, 125830, 123938, 126808, 13, 132730, 68785, 126298, 124172, 72804, 128342, 130918, 125013, 128510, 125077, 56794, 81778, 25871, 43982, 124839, 73441, 39434, 139068, 126212, 127519, 94957, 127564, 47632, 129726, 73441, 382, 124227, 39434, 125638, 124148, 133453, 73441, 130918, 125013, 128248, 124080, 124210, 73441, 134430, 73441, 129346, 68785, 126420, 39434, 133498, 138194, 127837, 39434, 69682, 125194, 124006, 128248, 139542, 124104, 136355, 124072, 125845, 73441, 13, 39434, 64604, 129425, 124058, 29825, 41593, 124863, 47632, 130918, 73441, 77273, 39434, 124552, 95198, 138750, 37524, 47632, 35244, 125908, 125993, 124669, 68785, 134607, 77273, 130119, 128264, 77273, 134744, 13, 140922, 135673, 123890, 130322, 130918, 125013, 128769, 125169, 17166, 47632, 33090, 124642, 47632, 136251, 37524, 135188, 133320, 382, 130362, 126556, 128248, 128349, 68785, 135545, 130918, 125013, 128248, 126731, 125089, 39697, 94957, 133157, 93543, 132474, 79820, 68785, 128660, 130656, 126815, 11071, 125729, 77273, 43982, 124839, 128438, 123913, 127580, 94957, 125179, 13, 63237, 128374, 39434, 129484, 130918, 125013, 68785, 50243, 132554, 137146, 94957, 133157, 93543, 141602, 63237, 92381, 124325, 37524, 130633, 125088, 127863, 68785, 129943, 73274, 129520, 124104, 128248, 126901, 124466, 126196, 94957, 29825, 79820, 47632, 128438, 73441, 382, 124636, 124147, 14293, 49388, 68785, 126298, 124104, 125007, 50243, 46586, 141064, 125007, 130918, 125013, 134436, 142001, 23364, 124346, 76841, 91344, 73441, 68785, 126420, 128420, 23364, 16157, 124226, 68238, 125113, 73441, 130243, 124138, 77273, 129458, 127226, 129843, 126229, 124104, 128835, 125815, 124104, 123894, 124388, 73441, 37524, 125290, 126606, 124104, 77273, 23364, 131377, 25871, 125857, 79820, 47632, 131359, 624, 2, 5430, 510, 31115, 220, 16, 15, 34117, 5248, 62626, 4755, 504, 279, 2701, 16229, 21085, 13, 53591, 1070, 1105, 553, 16829, 11, 24503, 748, 71806, 11, 323, 2033, 882, 624, 2, 9258, 43781, 510, 4913, 3, 48485, 788, 5212, 14582, 788, 5212, 13193, 788, 5212, 7841, 4326, 788, 5212, 4684, 788, 330, 14582, 1467, 10465, 330, 2102, 788, 330, 14582, 2918, 497, 330, 1313, 788, 330, 917, 14345, 330, 7841, 61610, 788, 5212, 4684, 788, 330, 852, 315, 4226, 2606, 10465, 330, 3615, 788, 5212, 1313, 788, 330, 917, 14345, 330, 1065, 4353, 788, 220, 17, 11, 330, 2102, 788, 330, 14582, 37243, 497, 330, 1313, 788, 330, 1653, 14345, 330, 19928, 28534, 788, 5212, 4684, 788, 330, 785, 4396, 4226, 10465, 330, 2102, 788, 330, 33092, 21806, 497, 330, 1313, 788, 330, 917, 14345, 330, 13193, 788, 5212, 3, 1097, 788, 5869, 10749, 48485, 14, 14582, 7903, 497, 330, 4684, 788, 330, 3889, 66415, 8201, 1189, 38154, 330, 6279, 788, 4383, 7841, 4326, 497, 330, 7841, 61610, 497, 330, 19928, 28534, 497, 330, 13193, 7914, 330, 2102, 788, 330, 14582, 497, 330, 1313, 788, 330, 1700, 14345, 330, 14582, 7903, 788, 5212, 13193, 788, 5212, 72109, 788, 5212, 9018, 788, 4383, 25756, 18609, 497, 330, 36730, 497, 330, 26292, 497, 330, 21751, 3866, 7914, 330, 2102, 788, 330, 72109, 497, 330, 1313, 788, 330, 917, 9207, 2137, 330, 6279, 788, 4383, 72109, 7914, 330, 2102, 788, 330, 14582, 7903, 497, 330, 1313, 788, 330, 1700, 9207, 2137, 330, 13193, 788, 5212, 17348, 788, 5212, 4684, 788, 330, 852, 315, 20869, 48, 82, 497, 330, 3615, 788, 5212, 3, 1097, 788, 5869, 10749, 48485, 14, 14582, 14345, 330, 2102, 788, 330, 35249, 497, 330, 1313, 788, 330, 1653, 9207, 2137, 330, 6279, 788, 4383, 17348, 7914, 330, 2102, 788, 330, 14582, 1649, 497, 330, 1313, 788, 330, 1700, 63159, 2, 9258, 4718, 510, 73594, 2236, 151645, 198, 151644, 77091, 198, 73594, 2236, 198, 14129, 59, 77, 262, 7245, 17348, 11693, 93815, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 133453, 73441, 130918, 125013, 77273, 139542, 124104, 128438, 73441, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 125290, 126606, 77273, 94957, 133157, 93543, 125088, 43635, 123995, 65290, 77, 394, 7245, 14293, 129425, 129346, 77273, 17166, 131412, 65290, 77, 394, 7245, 134788, 124280, 129816, 65290, 77, 394, 7245, 80970, 45577, 123829, 124267, 129431, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 125290, 126606, 77273, 94957, 133157, 93543, 125088, 43635, 123995, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124636, 124793, 131132, 39434, 64604, 129425, 130918, 125013, 56794, 129196, 125100, 124075, 124130, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 31382, 130170, 65290, 77, 394, 7245, 31382, 16157, 124523, 124966, 65290, 77, 394, 7245, 31382, 126456, 65290, 77, 394, 7245, 125962, 125263, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 31382, 16157, 124523, 124966, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 124220, 58656, 128307, 39434, 130135, 16157, 130918, 125013, 77273, 12961, 137757, 125993, 124669, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 135188, 138750, 65290, 77, 394, 7245, 125962, 123993, 73441, 65290, 77, 394, 7245, 31382, 124706, 124388, 65290, 77, 394, 7245, 80970, 129366, 129046, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 135188, 138750, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 39434, 129520, 130918, 125013, 77273, 126731, 125089, 39697, 94957, 133157, 93543, 132474, 79820, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 124425, 123987, 124104, 50243, 128667, 128953, 130259, 65290, 77, 394, 7245, 125343, 123904, 5703, 137146, 94957, 133157, 93543, 141602, 63237, 92381, 124325, 65290, 77, 394, 7245, 124706, 129060, 23364, 16157, 124669, 124104, 124114, 126687, 65290, 77, 394, 7245, 14293, 124388, 8532, 63237, 130133, 17166, 131412, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 125343, 123904, 5703, 137146, 94957, 133157, 93543, 141602, 63237, 92381, 124325, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28497, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 85153, 124062, 55057, 126455, 125799, 47632, 137637, 125006, 125861, 125013, 77273, 94957, 132111, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 127329, 132474, 69423, 65290, 77, 394, 7245, 130099, 94957, 124130, 126413, 124058, 124184, 125756, 65290, 77, 394, 7245, 135868, 140591, 65290, 77, 394, 7245, 31382, 130438, 126196, 126249, 127025, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 130099, 94957, 124130, 126413, 124058, 124184, 125756, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28497, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 129289, 94957, 69682, 125194, 47632, 136355, 125006, 125861, 125013, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 125290, 127580, 129816, 65290, 77, 394, 7245, 135188, 133320, 65290, 77, 394, 7245, 131578, 134668, 65290, 77, 394, 7245, 125089, 124346, 139089, 125164, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 135188, 133320, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 125750, 130918, 125013, 39434, 125638, 124148, 128248, 124080, 124210, 73441, 134430, 73441, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 133206, 68785, 138787, 127837, 65290, 77, 394, 7245, 80970, 68785, 129046, 39434, 43635, 123938, 126808, 130759, 65290, 77, 394, 7245, 124011, 124009, 77703, 125767, 127837, 65290, 77, 394, 7245, 127300, 128954, 140691, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 80970, 68785, 129046, 39434, 43635, 123938, 126808, 130759, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 143229, 126212, 130918, 125013, 124072, 83827, 10176, 125168, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 80970, 39434, 126929, 140691, 65290, 77, 394, 7245, 31382, 83827, 10176, 125168, 126731, 123978, 13325, 128248, 133122, 53710, 124075, 57859, 14558, 65290, 77, 394, 7245, 31382, 83827, 10176, 125168, 128707, 125492, 125320, 128248, 123877, 131757, 65290, 77, 394, 7245, 135188, 126985, 129829, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 31382, 83827, 10176, 125168, 126731, 123978, 13325, 128248, 133122, 53710, 124075, 57859, 14558, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 39434, 124035, 124661, 130918, 125013, 128248, 17166, 124042, 125815, 123894, 124388, 73441, 125006, 132468, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 14293, 137019, 124006, 65290, 77, 394, 7245, 125290, 126606, 77273, 129458, 10176, 133624, 65290, 77, 394, 7245, 124425, 123987, 124006, 43982, 130757, 126510, 65290, 77, 394, 7245, 80970, 39434, 69682, 125194, 129046, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 125290, 126606, 77273, 129458, 10176, 133624, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 126761, 63237, 123877, 124983, 124917, 128438, 73441, 128261, 129015, 128742, 23364, 16157, 124669, 53710, 124075, 57859, 73441, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 31382, 132451, 25871, 65290, 77, 394, 7245, 31382, 124341, 10176, 123862, 128883, 12653, 124142, 27490, 55057, 65290, 77, 394, 7245, 130099, 135887, 126808, 65290, 77, 394, 7245, 124839, 128862, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 130099, 135887, 126808, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 51300, 77, 262, 2279, 59, 77, 11195, 73594, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a question generation expert\n",
            "You will be provided by a PDF content associated with a Pydantic scheme.,\n",
            "Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\n",
            "Do not generate any introduction or conclusion.<|im_end|>\n",
            "<|im_start|>user\n",
            "# PDF CONTENT:\n",
            "**عنوان: أهمية الرياضيات في حياتنا اليومية**\n",
            "\n",
            "تعتبر الرياضيات من أهم العلوم التي تسهم في تشكيل التفكير المنطقي والقدرة على حل المشكلات. تتواجد الرياضيات في جميع مجالات حياتنا اليومية، حيث نستخدمها في العديد من الأنشطة البسيطة مثل التسوق، حساب النفقات، وحتى عند إعداد الطعام. فعندما نذهب إلى السوبر ماركت، نحتاج إلى حساب التكلفة الإجمالية لأصناف متعددة، وهذا يتطلب مهارات رياضية دقيقة.\n",
            "\n",
            "بالإضافة إلى ذلك، تُستخدم الرياضيات في مجالات أكثر تعقيدًا مثل الهندسة والبرمجة. يدرس المهندسون الرياضيات لبناء الهياكل والتصميمات، بينما يعتمد المبرمجون على أسس رياضية لتطوير البرمجيات والتطبيقات. لذلك، يمكن القول إن الرياضيات تمثل لغة عالمية تربط بين مختلف التخصصات العلمية.\n",
            "\n",
            "لم تقتصر أهمية الرياضيات على الناحية النظرية فقط، بل تشمل أيضًا تأثيرها على حياتنا الاجتماعية والاقتصادية. تُستخدم الإحصائيات الرياضية في تحليل البيانات واتخاذ القرارات، سواء في الحكومة أو في الشركات. يستطيع المسؤولون استخدام الرياضيات لتوقع الاتجاهات الاقتصادية وتحليل السوق.\n",
            "\n",
            "علاوة على ذلك، تعمل الرياضيات على تعزيز التفكير النقدي، وهو أمر ضروري في عالم اليوم السريع التغير. من خلال تعلم الرياضيات، نتعلم كيفية التفكير بطريقة منظمة وتطبيق المنطق، مما يساعدنا على التعامل مع التحديات اليومية.\n",
            "\n",
            "في الختام، يمكننا أن نستنتج أن الرياضيات ليست مجرد مادة دراسية، بل هي مهارة حيوية تسهم في تنمية شخصيتنا وقدراتنا العقلية وتساعدنا في مواجهة تحديات الحياة.\n",
            "# Task:\n",
            "Generate 10 Arabic multiple-choice questions from the following educational passage. Distribute them by difficulty, Bloom’s taxonomy, and response time.\n",
            "# Output Scheme:\n",
            "{\"$defs\": {\"Question\": {\"properties\": {\"question_text\": {\"description\": \"Question text.\", \"title\": \"Question Text\", \"type\": \"string\"}, \"question_answers\": {\"description\": \"List of answer options.\", \"items\": {\"type\": \"string\"}, \"minItems\": 2, \"title\": \"Question Answers\", \"type\": \"array\"}, \"correct_answer\": {\"description\": \"The correct answer.\", \"title\": \"Correct Answer\", \"type\": \"string\"}, \"properties\": {\"$ref\": \"#/$defs/QuestionProperties\", \"description\": \"Per-question attributes.\"}}, \"required\": [\"question_text\", \"question_answers\", \"correct_answer\", \"properties\"], \"title\": \"Question\", \"type\": \"object\"}, \"QuestionProperties\": {\"properties\": {\"Difficulty\": {\"enum\": [\"Very Easy\", \"Easy\", \"Average\", \"Difficult\"], \"title\": \"Difficulty\", \"type\": \"string\"}}, \"required\": [\"Difficulty\"], \"title\": \"QuestionProperties\", \"type\": \"object\"}}, \"properties\": {\"questions\": {\"description\": \"List of MCQs\", \"items\": {\"$ref\": \"#/$defs/Question\"}, \"title\": \"Questions\", \"type\": \"array\"}}, \"required\": [\"questions\"], \"title\": \"QuestionSet\", \"type\": \"object\"}\n",
            "\n",
            "# Output JSON:\n",
            "```json<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "\"{\\n    \\\"questions\\\": [\\n        {\\n            \\\"question_text\\\": \\\"ما هي أهمية الرياضيات في حياتنا اليومية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تساعد في التفكير المنطقي\\\",\\n                \\\"تستخدم فقط في الدراسة\\\",\\n                \\\"تضيع الوقت\\\",\\n                \\\"لا فائدة منها\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تساعد في التفكير المنطقي\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"في أي مجال تُستخدم الرياضيات لبناء الهياكل؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"الطب\\\",\\n                \\\"الهندسة\\\",\\n                \\\"الفن\\\",\\n                \\\"التاريخ\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"الهندسة\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو الدور الذي تلعبه الرياضيات في اتخاذ القرارات؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تحليل البيانات\\\",\\n                \\\"التسلية\\\",\\n                \\\"التنقل\\\",\\n                \\\"لا دور لها\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تحليل البيانات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف تساعد الرياضيات في تعزيز التفكير النقدي؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تجعلنا نكتب بشكل أفضل\\\",\\n                \\\"تعلمنا كيفية التفكير بطريقة منظمة\\\",\\n                \\\"تنمي مهاراتنا الفنية\\\",\\n                \\\"تقلل من وقت الدراسة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تعلمنا كيفية التفكير بطريقة منظمة\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Apply\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هي إحدى الاستخدامات العملية للرياضيات في التسوق؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"عداد النقود\\\",\\n                \\\"حساب التكلفة الإجمالية\\\",\\n                \\\"إعداد الطعام\\\",\\n                \\\"التحدث مع البائع\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"حساب التكلفة الإجمالية\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Apply\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو أحد التأثيرات الاجتماعية للرياضيات؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تسريع الوقت\\\",\\n                \\\"تحليل السوق\\\",\\n                \\\"تعليم الأطفال\\\",\\n                \\\"زيادة الأسعار\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تحليل السوق\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"هل الرياضيات تقتصر على الناحية النظرية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"نعم، تمامًا\\\",\\n                \\\"لا، لها تطبيقات عملية\\\",\\n                \\\"ربما قليلًا\\\",\\n                \\\"ليس هناك علاقة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"لا، لها تطبيقات عملية\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هي العلاقة بين الرياضيات والبرمجة؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"لا توجد علاقة\\\",\\n                \\\"البرمجة تعتمد على أساس رياضي\\\",\\n                \\\"البرمجة مرتكزة على الأدب\\\",\\n                \\\"تحليل الكلمات\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"البرمجة تعتمد على أساس رياضي\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف تؤثر الرياضيات على القدرات العقلية للفرد؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تضعفها\\\",\\n                \\\"تساعد في تنميتها\\\",\\n                \\\"تجعلها عشوائية\\\",\\n                \\\"لا تأثير لها\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تساعد في تنميتها\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو واحد من الأنشطة اليومية التي تتطلب مهارات رياضية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"القراءة\\\",\\n                \\\"الاستماع للموسيقى\\\",\\n                \\\"حساب النفقات\\\",\\n                \\\"المشي\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"حساب النفقات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        }\\n    ]\\n}\"\n",
            "```<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 14129, 59, 77, 262, 7245, 17348, 11693, 93815, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 133453, 73441, 130918, 125013, 77273, 139542, 124104, 128438, 73441, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 125290, 126606, 77273, 94957, 133157, 93543, 125088, 43635, 123995, 65290, 77, 394, 7245, 14293, 129425, 129346, 77273, 17166, 131412, 65290, 77, 394, 7245, 134788, 124280, 129816, 65290, 77, 394, 7245, 80970, 45577, 123829, 124267, 129431, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 125290, 126606, 77273, 94957, 133157, 93543, 125088, 43635, 123995, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124636, 124793, 131132, 39434, 64604, 129425, 130918, 125013, 56794, 129196, 125100, 124075, 124130, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 31382, 130170, 65290, 77, 394, 7245, 31382, 16157, 124523, 124966, 65290, 77, 394, 7245, 31382, 126456, 65290, 77, 394, 7245, 125962, 125263, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 31382, 16157, 124523, 124966, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 124220, 58656, 128307, 39434, 130135, 16157, 130918, 125013, 77273, 12961, 137757, 125993, 124669, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 135188, 138750, 65290, 77, 394, 7245, 125962, 123993, 73441, 65290, 77, 394, 7245, 31382, 124706, 124388, 65290, 77, 394, 7245, 80970, 129366, 129046, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 135188, 138750, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 36730, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28590, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 12472, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 39434, 129520, 130918, 125013, 77273, 126731, 125089, 39697, 94957, 133157, 93543, 132474, 79820, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 124425, 123987, 124104, 50243, 128667, 128953, 130259, 65290, 77, 394, 7245, 125343, 123904, 5703, 137146, 94957, 133157, 93543, 141602, 63237, 92381, 124325, 65290, 77, 394, 7245, 124706, 129060, 23364, 16157, 124669, 124104, 124114, 126687, 65290, 77, 394, 7245, 14293, 124388, 8532, 63237, 130133, 17166, 131412, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 125343, 123904, 5703, 137146, 94957, 133157, 93543, 141602, 63237, 92381, 124325, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28497, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 85153, 124062, 55057, 126455, 125799, 47632, 137637, 125006, 125861, 125013, 77273, 94957, 132111, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 127329, 132474, 69423, 65290, 77, 394, 7245, 130099, 94957, 124130, 126413, 124058, 124184, 125756, 65290, 77, 394, 7245, 135868, 140591, 65290, 77, 394, 7245, 31382, 130438, 126196, 126249, 127025, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 130099, 94957, 124130, 126413, 124058, 124184, 125756, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 28497, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 129289, 94957, 69682, 125194, 47632, 136355, 125006, 125861, 125013, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 125290, 127580, 129816, 65290, 77, 394, 7245, 135188, 133320, 65290, 77, 394, 7245, 131578, 134668, 65290, 77, 394, 7245, 125089, 124346, 139089, 125164, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 135188, 133320, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 125750, 130918, 125013, 39434, 125638, 124148, 128248, 124080, 124210, 73441, 134430, 73441, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 133206, 68785, 138787, 127837, 65290, 77, 394, 7245, 80970, 68785, 129046, 39434, 43635, 123938, 126808, 130759, 65290, 77, 394, 7245, 124011, 124009, 77703, 125767, 127837, 65290, 77, 394, 7245, 127300, 128954, 140691, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 80970, 68785, 129046, 39434, 43635, 123938, 126808, 130759, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128420, 143229, 126212, 130918, 125013, 124072, 83827, 10176, 125168, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 80970, 39434, 126929, 140691, 65290, 77, 394, 7245, 31382, 83827, 10176, 125168, 126731, 123978, 13325, 128248, 133122, 53710, 124075, 57859, 14558, 65290, 77, 394, 7245, 31382, 83827, 10176, 125168, 128707, 125492, 125320, 128248, 123877, 131757, 65290, 77, 394, 7245, 135188, 126985, 129829, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 31382, 83827, 10176, 125168, 126731, 123978, 13325, 128248, 133122, 53710, 124075, 57859, 14558, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 132558, 39434, 124035, 124661, 130918, 125013, 128248, 17166, 124042, 125815, 123894, 124388, 73441, 125006, 132468, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 14293, 137019, 124006, 65290, 77, 394, 7245, 125290, 126606, 77273, 129458, 10176, 133624, 65290, 77, 394, 7245, 124425, 123987, 124006, 43982, 130757, 126510, 65290, 77, 394, 7245, 80970, 39434, 69682, 125194, 129046, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 125290, 126606, 77273, 129458, 10176, 133624, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 21751, 3866, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 6583, 22245, 77, 310, 51300, 77, 286, 2470, 59, 77, 286, 28152, 77, 310, 7245, 7841, 4326, 11693, 7245, 124009, 128321, 126761, 63237, 123877, 124983, 124917, 128438, 73441, 128261, 129015, 128742, 23364, 16157, 124669, 53710, 124075, 57859, 73441, 128332, 65290, 77, 310, 7245, 7841, 61610, 11693, 93815, 77, 394, 7245, 31382, 132451, 25871, 65290, 77, 394, 7245, 31382, 124341, 10176, 123862, 128883, 12653, 124142, 27490, 55057, 65290, 77, 394, 7245, 130099, 135887, 126808, 65290, 77, 394, 7245, 124839, 128862, 22245, 77, 310, 10654, 59, 77, 310, 7245, 19928, 28534, 11693, 7245, 130099, 135887, 126808, 65290, 77, 310, 7245, 13193, 11693, 28152, 77, 394, 7245, 72109, 11693, 7245, 26292, 65290, 77, 394, 7245, 33, 18474, 15190, 16974, 11693, 7245, 82345, 65290, 77, 394, 7245, 2582, 4120, 11693, 7245, 40994, 22245, 77, 310, 51300, 77, 286, 51300, 77, 262, 2279, 59, 77, 11195, 73594, 151645, 198]\n",
            "labels:\n",
            "```json\n",
            "\"{\\n    \\\"questions\\\": [\\n        {\\n            \\\"question_text\\\": \\\"ما هي أهمية الرياضيات في حياتنا اليومية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تساعد في التفكير المنطقي\\\",\\n                \\\"تستخدم فقط في الدراسة\\\",\\n                \\\"تضيع الوقت\\\",\\n                \\\"لا فائدة منها\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تساعد في التفكير المنطقي\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"في أي مجال تُستخدم الرياضيات لبناء الهياكل؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"الطب\\\",\\n                \\\"الهندسة\\\",\\n                \\\"الفن\\\",\\n                \\\"التاريخ\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"الهندسة\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو الدور الذي تلعبه الرياضيات في اتخاذ القرارات؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تحليل البيانات\\\",\\n                \\\"التسلية\\\",\\n                \\\"التنقل\\\",\\n                \\\"لا دور لها\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تحليل البيانات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Easy\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Remember\\\",\\n                \\\"Response Time\\\": \\\"Short\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف تساعد الرياضيات في تعزيز التفكير النقدي؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تجعلنا نكتب بشكل أفضل\\\",\\n                \\\"تعلمنا كيفية التفكير بطريقة منظمة\\\",\\n                \\\"تنمي مهاراتنا الفنية\\\",\\n                \\\"تقلل من وقت الدراسة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تعلمنا كيفية التفكير بطريقة منظمة\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Apply\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هي إحدى الاستخدامات العملية للرياضيات في التسوق؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"عداد النقود\\\",\\n                \\\"حساب التكلفة الإجمالية\\\",\\n                \\\"إعداد الطعام\\\",\\n                \\\"التحدث مع البائع\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"حساب التكلفة الإجمالية\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Apply\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو أحد التأثيرات الاجتماعية للرياضيات؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تسريع الوقت\\\",\\n                \\\"تحليل السوق\\\",\\n                \\\"تعليم الأطفال\\\",\\n                \\\"زيادة الأسعار\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تحليل السوق\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"هل الرياضيات تقتصر على الناحية النظرية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"نعم، تمامًا\\\",\\n                \\\"لا، لها تطبيقات عملية\\\",\\n                \\\"ربما قليلًا\\\",\\n                \\\"ليس هناك علاقة\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"لا، لها تطبيقات عملية\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هي العلاقة بين الرياضيات والبرمجة؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"لا توجد علاقة\\\",\\n                \\\"البرمجة تعتمد على أساس رياضي\\\",\\n                \\\"البرمجة مرتكزة على الأدب\\\",\\n                \\\"تحليل الكلمات\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"البرمجة تعتمد على أساس رياضي\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"كيف تؤثر الرياضيات على القدرات العقلية للفرد؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"تضعفها\\\",\\n                \\\"تساعد في تنميتها\\\",\\n                \\\"تجعلها عشوائية\\\",\\n                \\\"لا تأثير لها\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"تساعد في تنميتها\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Difficult\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Long\\\"\\n            }\\n        },\\n        {\\n            \\\"question_text\\\": \\\"ما هو واحد من الأنشطة اليومية التي تتطلب مهارات رياضية؟\\\",\\n            \\\"question_answers\\\": [\\n                \\\"القراءة\\\",\\n                \\\"الاستماع للموسيقى\\\",\\n                \\\"حساب النفقات\\\",\\n                \\\"المشي\\\"\\n            ],\\n            \\\"correct_answer\\\": \\\"حساب النفقات\\\",\\n            \\\"properties\\\": {\\n                \\\"Difficulty\\\": \\\"Average\\\",\\n                \\\"Bloom Taxonomy\\\": \\\"Evaluate\\\",\\n                \\\"Response Time\\\": \\\"Medium\\\"\\n            }\\n        }\\n    ]\\n}\"\n",
            "```<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:752] 2025-08-10 07:08:47,527 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-10 07:08:47,528 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-08-10 07:08:47] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "model.safetensors: 100% 3.09G/3.09G [01:16<00:00, 40.2MB/s]\n",
            "[INFO|modeling_utils.py:1308] 2025-08-10 07:10:06,191 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/model.safetensors\n",
            "[INFO|modeling_utils.py:2411] 2025-08-10 07:10:06,201 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1098] 2025-08-10 07:10:06,204 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:5606] 2025-08-10 07:10:15,782 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5614] 2025-08-10 07:10:15,782 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 1.22MB/s]\n",
            "[INFO|configuration_utils.py:1053] 2025-08-10 07:10:16,089 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/generation_config.json\n",
            "[INFO|configuration_utils.py:1098] 2025-08-10 07:10:16,089 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-08-10 07:10:16] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-08-10 07:10:16] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-08-10 07:10:16] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-08-10 07:10:16] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-08-10 07:10:16] llamafactory.model.model_utils.misc:143 >> Found linear modules: o_proj,up_proj,k_proj,down_proj,q_proj,gate_proj,v_proj\n",
            "[INFO|2025-08-10 07:10:18] llamafactory.model.loader:143 >> trainable params: 73,859,072 || all params: 1,617,573,376 || trainable%: 4.5660\n",
            "[INFO|trainer.py:757] 2025-08-10 07:10:18,612 >> Using auto half precision backend\n",
            "[WARNING|2025-08-10 07:10:18] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.\n",
            "[INFO|trainer.py:2850] 2025-08-10 07:10:18,821 >> Loading model from /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-550.\n",
            "[INFO|trainer.py:2433] 2025-08-10 07:10:39,040 >> ***** Running training *****\n",
            "[INFO|trainer.py:2434] 2025-08-10 07:10:39,040 >>   Num examples = 1,000\n",
            "[INFO|trainer.py:2435] 2025-08-10 07:10:39,040 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2436] 2025-08-10 07:10:39,040 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2439] 2025-08-10 07:10:39,040 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:2440] 2025-08-10 07:10:39,040 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2441] 2025-08-10 07:10:39,040 >>   Total optimization steps = 750\n",
            "[INFO|trainer.py:2442] 2025-08-10 07:10:39,045 >>   Number of trainable parameters = 73,859,072\n",
            "[INFO|trainer.py:2464] 2025-08-10 07:10:39,048 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:2465] 2025-08-10 07:10:39,048 >>   Continuing training from epoch 2\n",
            "[INFO|trainer.py:2466] 2025-08-10 07:10:39,048 >>   Continuing training from global step 550\n",
            "[INFO|trainer.py:2468] 2025-08-10 07:10:39,048 >>   Will skip the first 2 epochs then the first 200 batches in the first epoch.\n",
            "[INFO|integration_utils.py:866] 2025-08-10 07:10:39,053 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristeenhallak33\u001b[0m (\u001b[33mchristeenhallak33-coretech-mena\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250810_071039-9bug1y1a\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mquestions_generations_Qwen\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/christeenhallak33-coretech-mena/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/christeenhallak33-coretech-mena/llamafactory/runs/9bug1y1a\u001b[0m\n",
            "{'loss': 0.2231, 'grad_norm': 0.3058423697948456, 'learning_rate': 1.8488789238604677e-05, 'epoch': 2.24}\n",
            "{'loss': 0.2256, 'grad_norm': 0.3330325186252594, 'learning_rate': 1.671676907308018e-05, 'epoch': 2.28}\n",
            "{'loss': 0.224, 'grad_norm': 0.35042187571525574, 'learning_rate': 1.5016832974331724e-05, 'epoch': 2.32}\n",
            "{'loss': 0.2096, 'grad_norm': 0.30595073103904724, 'learning_rate': 1.3392662625412488e-05, 'epoch': 2.36}\n",
            "{'loss': 0.2132, 'grad_norm': 0.3433358073234558, 'learning_rate': 1.1847775617632744e-05, 'epoch': 2.4}\n",
            " 80% 600/750 [35:11<1:45:58, 42.39s/it][INFO|trainer.py:4408] 2025-08-10 07:45:53,302 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4410] 2025-08-10 07:45:53,302 >>   Num examples = 175\n",
            "[INFO|trainer.py:4413] 2025-08-10 07:45:53,303 >>   Batch size = 1\n",
            "\n",
            "  0% 0/175 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/175 [00:03<04:48,  1.67s/it]\u001b[A\n",
            "  2% 3/175 [00:06<06:37,  2.31s/it]\u001b[A\n",
            "  2% 4/175 [00:10<08:01,  2.81s/it]\u001b[A\n",
            "  3% 5/175 [00:13<08:35,  3.03s/it]\u001b[A\n",
            "  3% 6/175 [00:17<08:56,  3.17s/it]\u001b[A\n",
            "  4% 7/175 [00:20<09:20,  3.34s/it]\u001b[A\n",
            "  5% 8/175 [00:24<09:28,  3.40s/it]\u001b[A\n",
            "  5% 9/175 [00:27<09:26,  3.41s/it]\u001b[A\n",
            "  6% 10/175 [00:31<09:19,  3.39s/it]\u001b[A\n",
            "  6% 11/175 [00:34<09:25,  3.45s/it]\u001b[A\n",
            "  7% 12/175 [00:38<09:38,  3.55s/it]\u001b[A\n",
            "  7% 13/175 [00:41<09:20,  3.46s/it]\u001b[A\n",
            "  8% 14/175 [00:45<09:17,  3.46s/it]\u001b[A\n",
            "  9% 15/175 [00:48<09:13,  3.46s/it]\u001b[A\n",
            "  9% 16/175 [00:52<09:26,  3.56s/it]\u001b[A\n",
            " 10% 17/175 [00:55<09:17,  3.53s/it]\u001b[A\n",
            " 10% 18/175 [00:58<08:49,  3.37s/it]\u001b[A\n",
            " 11% 19/175 [01:02<09:05,  3.50s/it]\u001b[A\n",
            " 11% 20/175 [01:06<09:16,  3.59s/it]\u001b[A\n",
            " 12% 21/175 [01:10<09:11,  3.58s/it]\u001b[A\n",
            " 13% 22/175 [01:13<08:57,  3.52s/it]\u001b[A\n",
            " 13% 23/175 [01:16<08:24,  3.32s/it]\u001b[A\n",
            " 14% 24/175 [01:19<08:18,  3.30s/it]\u001b[A\n",
            " 14% 25/175 [01:23<08:30,  3.41s/it]\u001b[A\n",
            " 15% 26/175 [01:26<08:23,  3.38s/it]\u001b[A\n",
            " 15% 27/175 [01:30<08:28,  3.43s/it]\u001b[A\n",
            " 16% 28/175 [01:33<08:25,  3.44s/it]\u001b[A\n",
            " 17% 29/175 [01:36<08:12,  3.37s/it]\u001b[A\n",
            " 17% 30/175 [01:40<08:12,  3.40s/it]\u001b[A\n",
            " 18% 31/175 [01:43<08:15,  3.44s/it]\u001b[A\n",
            " 18% 32/175 [01:47<08:12,  3.44s/it]\u001b[A\n",
            " 19% 33/175 [01:50<08:13,  3.47s/it]\u001b[A\n",
            " 19% 34/175 [01:54<08:28,  3.60s/it]\u001b[A\n",
            " 20% 35/175 [01:58<08:17,  3.55s/it]\u001b[A\n",
            " 21% 36/175 [02:01<08:09,  3.52s/it]\u001b[A\n",
            " 21% 37/175 [02:05<08:03,  3.51s/it]\u001b[A\n",
            " 22% 38/175 [02:08<07:53,  3.46s/it]\u001b[A\n",
            " 22% 39/175 [02:11<07:49,  3.45s/it]\u001b[A\n",
            " 23% 40/175 [02:15<07:49,  3.48s/it]\u001b[A\n",
            " 23% 41/175 [02:18<07:45,  3.47s/it]\u001b[A\n",
            " 24% 42/175 [02:22<07:45,  3.50s/it]\u001b[A\n",
            " 25% 43/175 [02:25<07:26,  3.38s/it]\u001b[A\n",
            " 25% 44/175 [02:29<07:30,  3.44s/it]\u001b[A\n",
            " 26% 45/175 [02:32<07:35,  3.50s/it]\u001b[A\n",
            " 26% 46/175 [02:36<07:38,  3.56s/it]\u001b[A\n",
            " 27% 47/175 [02:39<07:35,  3.56s/it]\u001b[A\n",
            " 27% 48/175 [02:43<07:31,  3.55s/it]\u001b[A\n",
            " 28% 49/175 [02:47<07:28,  3.56s/it]\u001b[A\n",
            " 29% 50/175 [02:50<07:08,  3.43s/it]\u001b[A\n",
            " 29% 51/175 [02:53<07:13,  3.50s/it]\u001b[A\n",
            " 30% 52/175 [02:57<07:11,  3.51s/it]\u001b[A\n",
            " 30% 53/175 [03:01<07:19,  3.60s/it]\u001b[A\n",
            " 31% 54/175 [03:04<07:01,  3.49s/it]\u001b[A\n",
            " 31% 55/175 [03:07<06:57,  3.48s/it]\u001b[A\n",
            " 32% 56/175 [03:11<07:00,  3.54s/it]\u001b[A\n",
            " 33% 57/175 [03:14<06:50,  3.48s/it]\u001b[A\n",
            " 33% 58/175 [03:18<06:38,  3.40s/it]\u001b[A\n",
            " 34% 59/175 [03:21<06:36,  3.42s/it]\u001b[A\n",
            " 34% 60/175 [03:24<06:22,  3.32s/it]\u001b[A\n",
            " 35% 61/175 [03:27<06:18,  3.32s/it]\u001b[A\n",
            " 35% 62/175 [03:31<06:19,  3.36s/it]\u001b[A\n",
            " 36% 63/175 [03:34<06:23,  3.42s/it]\u001b[A\n",
            " 37% 64/175 [03:38<06:20,  3.43s/it]\u001b[A\n",
            " 37% 65/175 [03:41<06:17,  3.43s/it]\u001b[A\n",
            " 38% 66/175 [03:45<06:17,  3.47s/it]\u001b[A\n",
            " 38% 67/175 [03:48<06:13,  3.46s/it]\u001b[A\n",
            " 39% 68/175 [03:52<06:10,  3.46s/it]\u001b[A\n",
            " 39% 69/175 [03:55<06:06,  3.46s/it]\u001b[A\n",
            " 40% 70/175 [03:59<06:10,  3.53s/it]\u001b[A\n",
            " 41% 71/175 [04:02<06:07,  3.53s/it]\u001b[A\n",
            " 41% 72/175 [04:06<06:07,  3.57s/it]\u001b[A\n",
            " 42% 73/175 [04:09<05:56,  3.49s/it]\u001b[A\n",
            " 42% 74/175 [04:13<05:51,  3.48s/it]\u001b[A\n",
            " 43% 75/175 [04:16<05:42,  3.43s/it]\u001b[A\n",
            " 43% 76/175 [04:20<05:39,  3.43s/it]\u001b[A\n",
            " 44% 77/175 [04:23<05:36,  3.43s/it]\u001b[A\n",
            " 45% 78/175 [04:26<05:29,  3.40s/it]\u001b[A\n",
            " 45% 79/175 [04:30<05:24,  3.38s/it]\u001b[A\n",
            " 46% 80/175 [04:33<05:19,  3.37s/it]\u001b[A\n",
            " 46% 81/175 [04:37<05:18,  3.39s/it]\u001b[A\n",
            " 47% 82/175 [04:40<05:23,  3.48s/it]\u001b[A\n",
            " 47% 83/175 [04:44<05:21,  3.50s/it]\u001b[A\n",
            " 48% 84/175 [04:47<05:23,  3.55s/it]\u001b[A\n",
            " 49% 85/175 [04:51<05:16,  3.52s/it]\u001b[A\n",
            " 49% 86/175 [04:54<05:13,  3.52s/it]\u001b[A\n",
            " 50% 87/175 [04:58<05:10,  3.53s/it]\u001b[A\n",
            " 50% 88/175 [05:01<05:01,  3.47s/it]\u001b[A\n",
            " 51% 89/175 [05:05<05:00,  3.49s/it]\u001b[A\n",
            " 51% 90/175 [05:08<04:55,  3.48s/it]\u001b[A\n",
            " 52% 91/175 [05:12<04:46,  3.41s/it]\u001b[A\n",
            " 53% 92/175 [05:15<04:43,  3.42s/it]\u001b[A\n",
            " 53% 93/175 [05:18<04:30,  3.29s/it]\u001b[A\n",
            " 54% 94/175 [05:21<04:24,  3.27s/it]\u001b[A\n",
            " 54% 95/175 [05:24<04:22,  3.29s/it]\u001b[A\n",
            " 55% 96/175 [05:28<04:20,  3.30s/it]\u001b[A\n",
            " 55% 97/175 [05:31<04:23,  3.38s/it]\u001b[A\n",
            " 56% 98/175 [05:35<04:24,  3.43s/it]\u001b[A\n",
            " 57% 99/175 [05:38<04:19,  3.41s/it]\u001b[A\n",
            " 57% 100/175 [05:42<04:16,  3.42s/it]\u001b[A\n",
            " 58% 101/175 [05:45<04:08,  3.36s/it]\u001b[A\n",
            " 58% 102/175 [05:49<04:09,  3.42s/it]\u001b[A\n",
            " 59% 103/175 [05:52<04:04,  3.40s/it]\u001b[A\n",
            " 59% 104/175 [05:56<04:09,  3.51s/it]\u001b[A\n",
            " 60% 105/175 [05:59<04:06,  3.52s/it]\u001b[A\n",
            " 61% 106/175 [06:03<04:01,  3.50s/it]\u001b[A\n",
            " 61% 107/175 [06:06<03:54,  3.45s/it]\u001b[A\n",
            " 62% 108/175 [06:09<03:51,  3.45s/it]\u001b[A\n",
            " 62% 109/175 [06:13<03:43,  3.39s/it]\u001b[A\n",
            " 63% 110/175 [06:16<03:38,  3.37s/it]\u001b[A\n",
            " 63% 111/175 [06:20<03:41,  3.45s/it]\u001b[A\n",
            " 64% 112/175 [06:23<03:37,  3.46s/it]\u001b[A\n",
            " 65% 113/175 [06:27<03:36,  3.49s/it]\u001b[A\n",
            " 65% 114/175 [06:30<03:25,  3.37s/it]\u001b[A\n",
            " 66% 115/175 [06:33<03:23,  3.40s/it]\u001b[A\n",
            " 66% 116/175 [06:37<03:25,  3.48s/it]\u001b[A\n",
            " 67% 117/175 [06:40<03:21,  3.48s/it]\u001b[A\n",
            " 67% 118/175 [06:44<03:16,  3.44s/it]\u001b[A\n",
            " 68% 119/175 [06:47<03:09,  3.38s/it]\u001b[A\n",
            " 69% 120/175 [06:50<03:06,  3.40s/it]\u001b[A\n",
            " 69% 121/175 [06:54<03:00,  3.35s/it]\u001b[A\n",
            " 70% 122/175 [06:57<02:55,  3.31s/it]\u001b[A\n",
            " 70% 123/175 [07:00<02:56,  3.39s/it]\u001b[A\n",
            " 71% 124/175 [07:04<02:51,  3.37s/it]\u001b[A\n",
            " 71% 125/175 [07:07<02:47,  3.36s/it]\u001b[A\n",
            " 72% 126/175 [07:11<02:46,  3.39s/it]\u001b[A\n",
            " 73% 127/175 [07:14<02:46,  3.47s/it]\u001b[A\n",
            " 73% 128/175 [07:17<02:39,  3.40s/it]\u001b[A\n",
            " 74% 129/175 [07:21<02:36,  3.41s/it]\u001b[A\n",
            " 74% 130/175 [07:24<02:35,  3.46s/it]\u001b[A\n",
            " 75% 131/175 [07:28<02:34,  3.52s/it]\u001b[A\n",
            " 75% 132/175 [07:32<02:31,  3.53s/it]\u001b[A\n",
            " 76% 133/175 [07:35<02:29,  3.57s/it]\u001b[A\n",
            " 77% 134/175 [07:39<02:23,  3.50s/it]\u001b[A\n",
            " 77% 135/175 [07:42<02:19,  3.48s/it]\u001b[A\n",
            " 78% 136/175 [07:45<02:13,  3.41s/it]\u001b[A\n",
            " 78% 137/175 [07:49<02:08,  3.39s/it]\u001b[A\n",
            " 79% 138/175 [07:52<02:06,  3.41s/it]\u001b[A\n",
            " 79% 139/175 [07:55<02:00,  3.35s/it]\u001b[A\n",
            " 80% 140/175 [07:59<01:58,  3.38s/it]\u001b[A\n",
            " 81% 141/175 [08:02<01:54,  3.36s/it]\u001b[A\n",
            " 81% 142/175 [08:06<01:54,  3.45s/it]\u001b[A\n",
            " 82% 143/175 [08:09<01:46,  3.32s/it]\u001b[A\n",
            " 82% 144/175 [08:12<01:45,  3.40s/it]\u001b[A\n",
            " 83% 145/175 [08:16<01:42,  3.41s/it]\u001b[A\n",
            " 83% 146/175 [08:19<01:37,  3.35s/it]\u001b[A\n",
            " 84% 147/175 [08:23<01:35,  3.41s/it]\u001b[A\n",
            " 85% 148/175 [08:26<01:31,  3.38s/it]\u001b[A\n",
            " 85% 149/175 [08:29<01:25,  3.27s/it]\u001b[A\n",
            " 86% 150/175 [08:33<01:24,  3.39s/it]\u001b[A\n",
            " 86% 151/175 [08:36<01:21,  3.41s/it]\u001b[A\n",
            " 87% 152/175 [08:39<01:18,  3.42s/it]\u001b[A\n",
            " 87% 153/175 [08:43<01:15,  3.43s/it]\u001b[A\n",
            " 88% 154/175 [08:46<01:12,  3.44s/it]\u001b[A\n",
            " 89% 155/175 [08:50<01:08,  3.45s/it]\u001b[A\n",
            " 89% 156/175 [08:53<01:05,  3.45s/it]\u001b[A\n",
            " 90% 157/175 [08:57<01:01,  3.42s/it]\u001b[A\n",
            " 90% 158/175 [09:00<00:57,  3.40s/it]\u001b[A\n",
            " 91% 159/175 [09:03<00:54,  3.41s/it]\u001b[A\n",
            " 91% 160/175 [09:07<00:50,  3.39s/it]\u001b[A\n",
            " 92% 161/175 [09:10<00:47,  3.37s/it]\u001b[A\n",
            " 93% 162/175 [09:14<00:44,  3.43s/it]\u001b[A\n",
            " 93% 163/175 [09:17<00:39,  3.33s/it]\u001b[A\n",
            " 94% 164/175 [09:20<00:36,  3.33s/it]\u001b[A\n",
            " 94% 165/175 [09:24<00:33,  3.37s/it]\u001b[A\n",
            " 95% 166/175 [09:27<00:30,  3.42s/it]\u001b[A\n",
            " 95% 167/175 [09:30<00:26,  3.37s/it]\u001b[A\n",
            " 96% 168/175 [09:34<00:23,  3.42s/it]\u001b[A\n",
            " 97% 169/175 [09:37<00:20,  3.39s/it]\u001b[A\n",
            " 97% 170/175 [09:41<00:16,  3.37s/it]\u001b[A\n",
            " 98% 171/175 [09:44<00:13,  3.33s/it]\u001b[A\n",
            " 98% 172/175 [09:47<00:10,  3.40s/it]\u001b[A\n",
            " 99% 173/175 [09:51<00:06,  3.38s/it]\u001b[A\n",
            " 99% 174/175 [09:54<00:03,  3.33s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.32405388355255127, 'eval_runtime': 601.0285, 'eval_samples_per_second': 0.291, 'eval_steps_per_second': 0.291, 'epoch': 2.4}\n",
            " 80% 600/750 [45:12<1:45:58, 42.39s/it]\n",
            "100% 175/175 [09:57<00:00,  3.31s/it]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:4074] 2025-08-10 07:55:54,341 >> Saving model checkpoint to /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-600\n",
            "[INFO|configuration_utils.py:752] 2025-08-10 07:55:54,621 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-10 07:55:54,623 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 07:55:59,834 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-600/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 07:55:59,848 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 07:55:59,859 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-600/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 07:56:09,336 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 07:56:09,664 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 07:56:09,994 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/special_tokens_map.json\n",
            "{'loss': 0.2199, 'grad_norm': 0.33028775453567505, 'learning_rate': 1.0385517832240471e-05, 'epoch': 2.44}\n",
            "{'loss': 0.2188, 'grad_norm': 0.3454761207103729, 'learning_rate': 9.00905619398757e-06, 'epoch': 2.48}\n",
            "{'loss': 0.213, 'grad_norm': 0.3271148204803467, 'learning_rate': 7.72137181227608e-06, 'epoch': 2.52}\n",
            "{'loss': 0.216, 'grad_norm': 0.34946852922439575, 'learning_rate': 6.52525352473905e-06, 'epoch': 2.56}\n",
            "{'loss': 0.2162, 'grad_norm': 0.34784600138664246, 'learning_rate': 5.4232918572391765e-06, 'epoch': 2.6}\n",
            " 87% 650/750 [1:20:52<1:10:32, 42.32s/it][INFO|trainer.py:4408] 2025-08-10 08:31:33,661 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4410] 2025-08-10 08:31:33,661 >>   Num examples = 175\n",
            "[INFO|trainer.py:4413] 2025-08-10 08:31:33,661 >>   Batch size = 1\n",
            "\n",
            "  0% 0/175 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/175 [00:03<04:48,  1.67s/it]\u001b[A\n",
            "  2% 3/175 [00:06<06:37,  2.31s/it]\u001b[A\n",
            "  2% 4/175 [00:10<08:01,  2.82s/it]\u001b[A\n",
            "  3% 5/175 [00:13<08:34,  3.03s/it]\u001b[A\n",
            "  3% 6/175 [00:17<08:55,  3.17s/it]\u001b[A\n",
            "  4% 7/175 [00:20<09:19,  3.33s/it]\u001b[A\n",
            "  5% 8/175 [00:24<09:27,  3.40s/it]\u001b[A\n",
            "  5% 9/175 [00:27<09:25,  3.41s/it]\u001b[A\n",
            "  6% 10/175 [00:31<09:18,  3.39s/it]\u001b[A\n",
            "  6% 11/175 [00:34<09:24,  3.44s/it]\u001b[A\n",
            "  7% 12/175 [00:38<09:38,  3.55s/it]\u001b[A\n",
            "  7% 13/175 [00:41<09:19,  3.46s/it]\u001b[A\n",
            "  8% 14/175 [00:45<09:16,  3.46s/it]\u001b[A\n",
            "  9% 15/175 [00:48<09:13,  3.46s/it]\u001b[A\n",
            "  9% 16/175 [00:52<09:25,  3.56s/it]\u001b[A\n",
            " 10% 17/175 [00:55<09:17,  3.53s/it]\u001b[A\n",
            " 10% 18/175 [00:58<08:49,  3.37s/it]\u001b[A\n",
            " 11% 19/175 [01:02<09:05,  3.50s/it]\u001b[A\n",
            " 11% 20/175 [01:06<09:16,  3.59s/it]\u001b[A\n",
            " 12% 21/175 [01:10<09:11,  3.58s/it]\u001b[A\n",
            " 13% 22/175 [01:13<08:57,  3.51s/it]\u001b[A\n",
            " 13% 23/175 [01:16<08:24,  3.32s/it]\u001b[A\n",
            " 14% 24/175 [01:19<08:18,  3.30s/it]\u001b[A\n",
            " 14% 25/175 [01:23<08:30,  3.40s/it]\u001b[A\n",
            " 15% 26/175 [01:26<08:22,  3.38s/it]\u001b[A\n",
            " 15% 27/175 [01:30<08:27,  3.43s/it]\u001b[A\n",
            " 16% 28/175 [01:33<08:25,  3.44s/it]\u001b[A\n",
            " 17% 29/175 [01:36<08:12,  3.38s/it]\u001b[A\n",
            " 17% 30/175 [01:40<08:12,  3.40s/it]\u001b[A\n",
            " 18% 31/175 [01:43<08:15,  3.44s/it]\u001b[A\n",
            " 18% 32/175 [01:47<08:12,  3.44s/it]\u001b[A\n",
            " 19% 33/175 [01:50<08:12,  3.47s/it]\u001b[A\n",
            " 19% 34/175 [01:54<08:27,  3.60s/it]\u001b[A\n",
            " 20% 35/175 [01:57<08:16,  3.55s/it]\u001b[A\n",
            " 21% 36/175 [02:01<08:09,  3.52s/it]\u001b[A\n",
            " 21% 37/175 [02:04<08:03,  3.50s/it]\u001b[A\n",
            " 22% 38/175 [02:08<07:53,  3.46s/it]\u001b[A\n",
            " 22% 39/175 [02:11<07:49,  3.45s/it]\u001b[A\n",
            " 23% 40/175 [02:15<07:49,  3.48s/it]\u001b[A\n",
            " 23% 41/175 [02:18<07:45,  3.47s/it]\u001b[A\n",
            " 24% 42/175 [02:22<07:44,  3.49s/it]\u001b[A\n",
            " 25% 43/175 [02:25<07:26,  3.38s/it]\u001b[A\n",
            " 25% 44/175 [02:28<07:29,  3.43s/it]\u001b[A\n",
            " 26% 45/175 [02:32<07:35,  3.50s/it]\u001b[A\n",
            " 26% 46/175 [02:36<07:38,  3.55s/it]\u001b[A\n",
            " 27% 47/175 [02:39<07:34,  3.55s/it]\u001b[A\n",
            " 27% 48/175 [02:43<07:30,  3.55s/it]\u001b[A\n",
            " 28% 49/175 [02:46<07:27,  3.55s/it]\u001b[A\n",
            " 29% 50/175 [02:50<07:07,  3.42s/it]\u001b[A\n",
            " 29% 51/175 [02:53<07:12,  3.49s/it]\u001b[A\n",
            " 30% 52/175 [02:57<07:11,  3.51s/it]\u001b[A\n",
            " 30% 53/175 [03:01<07:18,  3.60s/it]\u001b[A\n",
            " 31% 54/175 [03:04<07:01,  3.49s/it]\u001b[A\n",
            " 31% 55/175 [03:07<06:57,  3.48s/it]\u001b[A\n",
            " 32% 56/175 [03:11<07:00,  3.54s/it]\u001b[A\n",
            " 33% 57/175 [03:14<06:50,  3.48s/it]\u001b[A\n",
            " 33% 58/175 [03:17<06:38,  3.41s/it]\u001b[A\n",
            " 34% 59/175 [03:21<06:36,  3.42s/it]\u001b[A\n",
            " 34% 60/175 [03:24<06:22,  3.33s/it]\u001b[A\n",
            " 35% 61/175 [03:27<06:19,  3.33s/it]\u001b[A\n",
            " 35% 62/175 [03:31<06:19,  3.36s/it]\u001b[A\n",
            " 36% 63/175 [03:34<06:24,  3.44s/it]\u001b[A\n",
            " 37% 64/175 [03:38<06:21,  3.44s/it]\u001b[A\n",
            " 37% 65/175 [03:41<06:18,  3.44s/it]\u001b[A\n",
            " 38% 66/175 [03:45<06:18,  3.47s/it]\u001b[A\n",
            " 38% 67/175 [03:48<06:14,  3.47s/it]\u001b[A\n",
            " 39% 68/175 [03:52<06:11,  3.47s/it]\u001b[A\n",
            " 39% 69/175 [03:55<06:07,  3.46s/it]\u001b[A\n",
            " 40% 70/175 [03:59<06:10,  3.53s/it]\u001b[A\n",
            " 41% 71/175 [04:02<06:07,  3.54s/it]\u001b[A\n",
            " 41% 72/175 [04:06<06:08,  3.58s/it]\u001b[A\n",
            " 42% 73/175 [04:09<05:57,  3.50s/it]\u001b[A\n",
            " 42% 74/175 [04:13<05:51,  3.48s/it]\u001b[A\n",
            " 43% 75/175 [04:16<05:43,  3.43s/it]\u001b[A\n",
            " 43% 76/175 [04:20<05:40,  3.44s/it]\u001b[A\n",
            " 44% 77/175 [04:23<05:36,  3.44s/it]\u001b[A\n",
            " 45% 78/175 [04:26<05:30,  3.40s/it]\u001b[A\n",
            " 45% 79/175 [04:30<05:25,  3.39s/it]\u001b[A\n",
            " 46% 80/175 [04:33<05:19,  3.37s/it]\u001b[A\n",
            " 46% 81/175 [04:37<05:18,  3.39s/it]\u001b[A\n",
            " 47% 82/175 [04:40<05:23,  3.48s/it]\u001b[A\n",
            " 47% 83/175 [04:44<05:22,  3.50s/it]\u001b[A\n",
            " 48% 84/175 [04:48<05:23,  3.56s/it]\u001b[A\n",
            " 49% 85/175 [04:51<05:17,  3.52s/it]\u001b[A\n",
            " 49% 86/175 [04:54<05:14,  3.53s/it]\u001b[A\n",
            " 50% 87/175 [04:58<05:11,  3.54s/it]\u001b[A\n",
            " 50% 88/175 [05:01<05:02,  3.47s/it]\u001b[A\n",
            " 51% 89/175 [05:05<05:00,  3.50s/it]\u001b[A\n",
            " 51% 90/175 [05:08<04:56,  3.49s/it]\u001b[A\n",
            " 52% 91/175 [05:12<04:46,  3.42s/it]\u001b[A\n",
            " 53% 92/175 [05:15<04:44,  3.43s/it]\u001b[A\n",
            " 53% 93/175 [05:18<04:30,  3.30s/it]\u001b[A\n",
            " 54% 94/175 [05:21<04:25,  3.28s/it]\u001b[A\n",
            " 54% 95/175 [05:25<04:23,  3.29s/it]\u001b[A\n",
            " 55% 96/175 [05:28<04:20,  3.30s/it]\u001b[A\n",
            " 55% 97/175 [05:32<04:23,  3.38s/it]\u001b[A\n",
            " 56% 98/175 [05:35<04:24,  3.44s/it]\u001b[A\n",
            " 57% 99/175 [05:38<04:19,  3.41s/it]\u001b[A\n",
            " 57% 100/175 [05:42<04:16,  3.42s/it]\u001b[A\n",
            " 58% 101/175 [05:45<04:09,  3.37s/it]\u001b[A\n",
            " 58% 102/175 [05:49<04:10,  3.43s/it]\u001b[A\n",
            " 59% 103/175 [05:52<04:05,  3.40s/it]\u001b[A\n",
            " 59% 104/175 [05:56<04:09,  3.52s/it]\u001b[A\n",
            " 60% 105/175 [05:59<04:06,  3.53s/it]\u001b[A\n",
            " 61% 106/175 [06:03<04:01,  3.50s/it]\u001b[A\n",
            " 61% 107/175 [06:06<03:55,  3.46s/it]\u001b[A\n",
            " 62% 108/175 [06:10<03:51,  3.46s/it]\u001b[A\n",
            " 62% 109/175 [06:13<03:43,  3.39s/it]\u001b[A\n",
            " 63% 110/175 [06:16<03:38,  3.37s/it]\u001b[A\n",
            " 63% 111/175 [06:20<03:41,  3.45s/it]\u001b[A\n",
            " 64% 112/175 [06:23<03:37,  3.46s/it]\u001b[A\n",
            " 65% 113/175 [06:27<03:36,  3.49s/it]\u001b[A\n",
            " 65% 114/175 [06:30<03:25,  3.37s/it]\u001b[A\n",
            " 66% 115/175 [06:33<03:23,  3.40s/it]\u001b[A\n",
            " 66% 116/175 [06:37<03:25,  3.48s/it]\u001b[A\n",
            " 67% 117/175 [06:41<03:21,  3.47s/it]\u001b[A\n",
            " 67% 118/175 [06:44<03:15,  3.44s/it]\u001b[A\n",
            " 68% 119/175 [06:47<03:09,  3.38s/it]\u001b[A\n",
            " 69% 120/175 [06:51<03:06,  3.40s/it]\u001b[A\n",
            " 69% 121/175 [06:54<03:00,  3.34s/it]\u001b[A\n",
            " 70% 122/175 [06:57<02:55,  3.31s/it]\u001b[A\n",
            " 70% 123/175 [07:01<02:56,  3.39s/it]\u001b[A\n",
            " 71% 124/175 [07:04<02:51,  3.37s/it]\u001b[A\n",
            " 71% 125/175 [07:07<02:47,  3.36s/it]\u001b[A\n",
            " 72% 126/175 [07:11<02:45,  3.39s/it]\u001b[A\n",
            " 73% 127/175 [07:14<02:46,  3.47s/it]\u001b[A\n",
            " 73% 128/175 [07:18<02:39,  3.39s/it]\u001b[A\n",
            " 74% 129/175 [07:21<02:36,  3.41s/it]\u001b[A\n",
            " 74% 130/175 [07:25<02:35,  3.46s/it]\u001b[A\n",
            " 75% 131/175 [07:28<02:34,  3.52s/it]\u001b[A\n",
            " 75% 132/175 [07:32<02:31,  3.53s/it]\u001b[A\n",
            " 76% 133/175 [07:35<02:29,  3.57s/it]\u001b[A\n",
            " 77% 134/175 [07:39<02:23,  3.49s/it]\u001b[A\n",
            " 77% 135/175 [07:42<02:19,  3.48s/it]\u001b[A\n",
            " 78% 136/175 [07:45<02:12,  3.41s/it]\u001b[A\n",
            " 78% 137/175 [07:49<02:08,  3.38s/it]\u001b[A\n",
            " 79% 138/175 [07:52<02:05,  3.40s/it]\u001b[A\n",
            " 79% 139/175 [07:55<02:00,  3.35s/it]\u001b[A\n",
            " 80% 140/175 [07:59<01:58,  3.37s/it]\u001b[A\n",
            " 81% 141/175 [08:02<01:54,  3.35s/it]\u001b[A\n",
            " 81% 142/175 [08:06<01:53,  3.45s/it]\u001b[A\n",
            " 82% 143/175 [08:09<01:46,  3.31s/it]\u001b[A\n",
            " 82% 144/175 [08:12<01:45,  3.39s/it]\u001b[A\n",
            " 83% 145/175 [08:16<01:42,  3.41s/it]\u001b[A\n",
            " 83% 146/175 [08:19<01:37,  3.35s/it]\u001b[A\n",
            " 84% 147/175 [08:23<01:35,  3.40s/it]\u001b[A\n",
            " 85% 148/175 [08:26<01:31,  3.38s/it]\u001b[A\n",
            " 85% 149/175 [08:29<01:24,  3.26s/it]\u001b[A\n",
            " 86% 150/175 [08:33<01:24,  3.38s/it]\u001b[A\n",
            " 86% 151/175 [08:36<01:21,  3.40s/it]\u001b[A\n",
            " 87% 152/175 [08:40<01:18,  3.41s/it]\u001b[A\n",
            " 87% 153/175 [08:43<01:15,  3.43s/it]\u001b[A\n",
            " 88% 154/175 [08:46<01:12,  3.44s/it]\u001b[A\n",
            " 89% 155/175 [08:50<01:08,  3.44s/it]\u001b[A\n",
            " 89% 156/175 [08:53<01:05,  3.45s/it]\u001b[A\n",
            " 90% 157/175 [08:57<01:01,  3.41s/it]\u001b[A\n",
            " 90% 158/175 [09:00<00:57,  3.39s/it]\u001b[A\n",
            " 91% 159/175 [09:03<00:54,  3.41s/it]\u001b[A\n",
            " 91% 160/175 [09:07<00:50,  3.39s/it]\u001b[A\n",
            " 92% 161/175 [09:10<00:47,  3.37s/it]\u001b[A\n",
            " 93% 162/175 [09:14<00:44,  3.42s/it]\u001b[A\n",
            " 93% 163/175 [09:17<00:39,  3.32s/it]\u001b[A\n",
            " 94% 164/175 [09:20<00:36,  3.32s/it]\u001b[A\n",
            " 94% 165/175 [09:24<00:33,  3.36s/it]\u001b[A\n",
            " 95% 166/175 [09:27<00:30,  3.42s/it]\u001b[A\n",
            " 95% 167/175 [09:30<00:26,  3.36s/it]\u001b[A\n",
            " 96% 168/175 [09:34<00:23,  3.42s/it]\u001b[A\n",
            " 97% 169/175 [09:37<00:20,  3.39s/it]\u001b[A\n",
            " 97% 170/175 [09:40<00:16,  3.37s/it]\u001b[A\n",
            " 98% 171/175 [09:44<00:13,  3.33s/it]\u001b[A\n",
            " 98% 172/175 [09:47<00:10,  3.39s/it]\u001b[A\n",
            " 99% 173/175 [09:51<00:06,  3.38s/it]\u001b[A\n",
            " 99% 174/175 [09:54<00:03,  3.33s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.3230166733264923, 'eval_runtime': 600.9406, 'eval_samples_per_second': 0.291, 'eval_steps_per_second': 0.291, 'epoch': 2.6}\n",
            " 87% 650/750 [1:30:53<1:10:32, 42.32s/it]\n",
            "100% 175/175 [09:57<00:00,  3.30s/it]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:4074] 2025-08-10 08:41:34,612 >> Saving model checkpoint to /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-650\n",
            "[INFO|configuration_utils.py:752] 2025-08-10 08:41:34,875 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-10 08:41:34,877 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 08:41:36,255 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-650/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 08:41:36,261 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-650/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 08:41:36,267 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-650/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 08:41:40,864 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 08:41:40,876 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 08:41:40,881 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/special_tokens_map.json\n",
            "{'loss': 0.2142, 'grad_norm': 0.339128702878952, 'learning_rate': 4.417873413366702e-06, 'epoch': 2.64}\n",
            "{'loss': 0.2161, 'grad_norm': 0.3584739565849304, 'learning_rate': 3.511175705587433e-06, 'epoch': 2.68}\n",
            "{'loss': 0.2036, 'grad_norm': 0.35871076583862305, 'learning_rate': 2.7051624392356477e-06, 'epoch': 2.72}\n",
            "{'loss': 0.2132, 'grad_norm': 0.33852913975715637, 'learning_rate': 2.0015792595656226e-06, 'epoch': 2.76}\n",
            "{'loss': 0.2131, 'grad_norm': 0.3611070513725281, 'learning_rate': 1.4019499710726913e-06, 'epoch': 2.8}\n",
            " 93% 700/750 [2:06:04<34:46, 41.73s/it][INFO|trainer.py:4408] 2025-08-10 09:16:45,995 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4410] 2025-08-10 09:16:45,996 >>   Num examples = 175\n",
            "[INFO|trainer.py:4413] 2025-08-10 09:16:45,996 >>   Batch size = 1\n",
            "\n",
            "  0% 0/175 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/175 [00:03<04:48,  1.67s/it]\u001b[A\n",
            "  2% 3/175 [00:06<06:38,  2.32s/it]\u001b[A\n",
            "  2% 4/175 [00:10<08:02,  2.82s/it]\u001b[A\n",
            "  3% 5/175 [00:13<08:35,  3.03s/it]\u001b[A\n",
            "  3% 6/175 [00:17<08:56,  3.17s/it]\u001b[A\n",
            "  4% 7/175 [00:20<09:20,  3.34s/it]\u001b[A\n",
            "  5% 8/175 [00:24<09:28,  3.41s/it]\u001b[A\n",
            "  5% 9/175 [00:27<09:26,  3.41s/it]\u001b[A\n",
            "  6% 10/175 [00:31<09:19,  3.39s/it]\u001b[A\n",
            "  6% 11/175 [00:34<09:27,  3.46s/it]\u001b[A\n",
            "  7% 12/175 [00:38<09:40,  3.56s/it]\u001b[A\n",
            "  7% 13/175 [00:41<09:21,  3.46s/it]\u001b[A\n",
            "  8% 14/175 [00:45<09:17,  3.47s/it]\u001b[A\n",
            "  9% 15/175 [00:48<09:14,  3.47s/it]\u001b[A\n",
            "  9% 16/175 [00:52<09:26,  3.56s/it]\u001b[A\n",
            " 10% 17/175 [00:55<09:18,  3.53s/it]\u001b[A\n",
            " 10% 18/175 [00:58<08:49,  3.37s/it]\u001b[A\n",
            " 11% 19/175 [01:02<09:05,  3.50s/it]\u001b[A\n",
            " 11% 20/175 [01:06<09:16,  3.59s/it]\u001b[A\n",
            " 12% 21/175 [01:10<09:12,  3.58s/it]\u001b[A\n",
            " 13% 22/175 [01:13<08:57,  3.51s/it]\u001b[A\n",
            " 13% 23/175 [01:16<08:25,  3.32s/it]\u001b[A\n",
            " 14% 24/175 [01:19<08:18,  3.30s/it]\u001b[A\n",
            " 14% 25/175 [01:23<08:31,  3.41s/it]\u001b[A\n",
            " 15% 26/175 [01:26<08:23,  3.38s/it]\u001b[A\n",
            " 15% 27/175 [01:30<08:27,  3.43s/it]\u001b[A\n",
            " 16% 28/175 [01:33<08:26,  3.44s/it]\u001b[A\n",
            " 17% 29/175 [01:36<08:12,  3.38s/it]\u001b[A\n",
            " 17% 30/175 [01:40<08:12,  3.40s/it]\u001b[A\n",
            " 18% 31/175 [01:43<08:15,  3.44s/it]\u001b[A\n",
            " 18% 32/175 [01:47<08:12,  3.44s/it]\u001b[A\n",
            " 19% 33/175 [01:50<08:12,  3.47s/it]\u001b[A\n",
            " 19% 34/175 [01:54<08:27,  3.60s/it]\u001b[A\n",
            " 20% 35/175 [01:58<08:16,  3.55s/it]\u001b[A\n",
            " 21% 36/175 [02:01<08:09,  3.52s/it]\u001b[A\n",
            " 21% 37/175 [02:05<08:03,  3.51s/it]\u001b[A\n",
            " 22% 38/175 [02:08<07:53,  3.46s/it]\u001b[A\n",
            " 22% 39/175 [02:11<07:49,  3.45s/it]\u001b[A\n",
            " 23% 40/175 [02:15<07:49,  3.48s/it]\u001b[A\n",
            " 23% 41/175 [02:18<07:45,  3.47s/it]\u001b[A\n",
            " 24% 42/175 [02:22<07:44,  3.50s/it]\u001b[A\n",
            " 25% 43/175 [02:25<07:26,  3.38s/it]\u001b[A\n",
            " 25% 44/175 [02:29<07:30,  3.44s/it]\u001b[A\n",
            " 26% 45/175 [02:32<07:35,  3.51s/it]\u001b[A\n",
            " 26% 46/175 [02:36<07:39,  3.56s/it]\u001b[A\n",
            " 27% 47/175 [02:39<07:35,  3.56s/it]\u001b[A\n",
            " 27% 48/175 [02:43<07:31,  3.56s/it]\u001b[A\n",
            " 28% 49/175 [02:47<07:28,  3.56s/it]\u001b[A\n",
            " 29% 50/175 [02:50<07:08,  3.43s/it]\u001b[A\n",
            " 29% 51/175 [02:53<07:13,  3.50s/it]\u001b[A\n",
            " 30% 52/175 [02:57<07:11,  3.51s/it]\u001b[A\n",
            " 30% 53/175 [03:01<07:19,  3.60s/it]\u001b[A\n",
            " 31% 54/175 [03:04<07:02,  3.49s/it]\u001b[A\n",
            " 31% 55/175 [03:07<06:57,  3.48s/it]\u001b[A\n",
            " 32% 56/175 [03:11<07:00,  3.54s/it]\u001b[A\n",
            " 33% 57/175 [03:14<06:50,  3.48s/it]\u001b[A\n",
            " 33% 58/175 [03:18<06:38,  3.40s/it]\u001b[A\n",
            " 34% 59/175 [03:21<06:36,  3.42s/it]\u001b[A\n",
            " 34% 60/175 [03:24<06:22,  3.33s/it]\u001b[A\n",
            " 35% 61/175 [03:28<06:19,  3.32s/it]\u001b[A\n",
            " 35% 62/175 [03:31<06:19,  3.36s/it]\u001b[A\n",
            " 36% 63/175 [03:35<06:23,  3.43s/it]\u001b[A\n",
            " 37% 64/175 [03:38<06:20,  3.43s/it]\u001b[A\n",
            " 37% 65/175 [03:41<06:18,  3.44s/it]\u001b[A\n",
            " 38% 66/175 [03:45<06:18,  3.47s/it]\u001b[A\n",
            " 38% 67/175 [03:48<06:14,  3.47s/it]\u001b[A\n",
            " 39% 68/175 [03:52<06:10,  3.46s/it]\u001b[A\n",
            " 39% 69/175 [03:55<06:06,  3.46s/it]\u001b[A\n",
            " 40% 70/175 [03:59<06:10,  3.53s/it]\u001b[A\n",
            " 41% 71/175 [04:03<06:07,  3.53s/it]\u001b[A\n",
            " 41% 72/175 [04:06<06:07,  3.57s/it]\u001b[A\n",
            " 42% 73/175 [04:10<05:56,  3.50s/it]\u001b[A\n",
            " 42% 74/175 [04:13<05:51,  3.48s/it]\u001b[A\n",
            " 43% 75/175 [04:16<05:43,  3.43s/it]\u001b[A\n",
            " 43% 76/175 [04:20<05:40,  3.44s/it]\u001b[A\n",
            " 44% 77/175 [04:23<05:36,  3.44s/it]\u001b[A\n",
            " 45% 78/175 [04:27<05:30,  3.40s/it]\u001b[A\n",
            " 45% 79/175 [04:30<05:24,  3.38s/it]\u001b[A\n",
            " 46% 80/175 [04:33<05:19,  3.37s/it]\u001b[A\n",
            " 46% 81/175 [04:37<05:18,  3.39s/it]\u001b[A\n",
            " 47% 82/175 [04:40<05:23,  3.48s/it]\u001b[A\n",
            " 47% 83/175 [04:44<05:21,  3.50s/it]\u001b[A\n",
            " 48% 84/175 [04:48<05:23,  3.55s/it]\u001b[A\n",
            " 49% 85/175 [04:51<05:16,  3.52s/it]\u001b[A\n",
            " 49% 86/175 [04:55<05:13,  3.53s/it]\u001b[A\n",
            " 50% 87/175 [04:58<05:10,  3.53s/it]\u001b[A\n",
            " 50% 88/175 [05:01<05:01,  3.47s/it]\u001b[A\n",
            " 51% 89/175 [05:05<05:00,  3.49s/it]\u001b[A\n",
            " 51% 90/175 [05:08<04:55,  3.48s/it]\u001b[A\n",
            " 52% 91/175 [05:12<04:46,  3.41s/it]\u001b[A\n",
            " 53% 92/175 [05:15<04:44,  3.42s/it]\u001b[A\n",
            " 53% 93/175 [05:18<04:30,  3.30s/it]\u001b[A\n",
            " 54% 94/175 [05:21<04:25,  3.27s/it]\u001b[A\n",
            " 54% 95/175 [05:25<04:23,  3.29s/it]\u001b[A\n",
            " 55% 96/175 [05:28<04:20,  3.30s/it]\u001b[A\n",
            " 55% 97/175 [05:32<04:23,  3.38s/it]\u001b[A\n",
            " 56% 98/175 [05:35<04:24,  3.44s/it]\u001b[A\n",
            " 57% 99/175 [05:39<04:19,  3.41s/it]\u001b[A\n",
            " 57% 100/175 [05:42<04:16,  3.42s/it]\u001b[A\n",
            " 58% 101/175 [05:45<04:09,  3.37s/it]\u001b[A\n",
            " 58% 102/175 [05:49<04:10,  3.43s/it]\u001b[A\n",
            " 59% 103/175 [05:52<04:04,  3.40s/it]\u001b[A\n",
            " 59% 104/175 [05:56<04:09,  3.52s/it]\u001b[A\n",
            " 60% 105/175 [05:59<04:06,  3.53s/it]\u001b[A\n",
            " 61% 106/175 [06:03<04:01,  3.51s/it]\u001b[A\n",
            " 61% 107/175 [06:06<03:55,  3.46s/it]\u001b[A\n",
            " 62% 108/175 [06:10<03:51,  3.46s/it]\u001b[A\n",
            " 62% 109/175 [06:13<03:43,  3.39s/it]\u001b[A\n",
            " 63% 110/175 [06:16<03:38,  3.37s/it]\u001b[A\n",
            " 63% 111/175 [06:20<03:41,  3.45s/it]\u001b[A\n",
            " 64% 112/175 [06:23<03:37,  3.46s/it]\u001b[A\n",
            " 65% 113/175 [06:27<03:36,  3.49s/it]\u001b[A\n",
            " 65% 114/175 [06:30<03:25,  3.37s/it]\u001b[A\n",
            " 66% 115/175 [06:33<03:23,  3.40s/it]\u001b[A\n",
            " 66% 116/175 [06:37<03:25,  3.48s/it]\u001b[A\n",
            " 67% 117/175 [06:41<03:21,  3.48s/it]\u001b[A\n",
            " 67% 118/175 [06:44<03:15,  3.44s/it]\u001b[A\n",
            " 68% 119/175 [06:47<03:09,  3.38s/it]\u001b[A\n",
            " 69% 120/175 [06:51<03:06,  3.40s/it]\u001b[A\n",
            " 69% 121/175 [06:54<03:00,  3.35s/it]\u001b[A\n",
            " 70% 122/175 [06:57<02:55,  3.32s/it]\u001b[A\n",
            " 70% 123/175 [07:01<02:56,  3.39s/it]\u001b[A\n",
            " 71% 124/175 [07:04<02:51,  3.37s/it]\u001b[A\n",
            " 71% 125/175 [07:07<02:48,  3.36s/it]\u001b[A\n",
            " 72% 126/175 [07:11<02:46,  3.39s/it]\u001b[A\n",
            " 73% 127/175 [07:14<02:46,  3.47s/it]\u001b[A\n",
            " 73% 128/175 [07:18<02:39,  3.40s/it]\u001b[A\n",
            " 74% 129/175 [07:21<02:36,  3.41s/it]\u001b[A\n",
            " 74% 130/175 [07:25<02:35,  3.46s/it]\u001b[A\n",
            " 75% 131/175 [07:28<02:35,  3.52s/it]\u001b[A\n",
            " 75% 132/175 [07:32<02:32,  3.54s/it]\u001b[A\n",
            " 76% 133/175 [07:36<02:30,  3.57s/it]\u001b[A\n",
            " 77% 134/175 [07:39<02:23,  3.50s/it]\u001b[A\n",
            " 77% 135/175 [07:42<02:19,  3.49s/it]\u001b[A\n",
            " 78% 136/175 [07:46<02:13,  3.42s/it]\u001b[A\n",
            " 78% 137/175 [07:49<02:08,  3.39s/it]\u001b[A\n",
            " 79% 138/175 [07:52<02:06,  3.41s/it]\u001b[A\n",
            " 79% 139/175 [07:56<02:00,  3.35s/it]\u001b[A\n",
            " 80% 140/175 [07:59<01:58,  3.38s/it]\u001b[A\n",
            " 81% 141/175 [08:02<01:54,  3.37s/it]\u001b[A\n",
            " 81% 142/175 [08:06<01:54,  3.46s/it]\u001b[A\n",
            " 82% 143/175 [08:09<01:46,  3.32s/it]\u001b[A\n",
            " 82% 144/175 [08:13<01:45,  3.40s/it]\u001b[A\n",
            " 83% 145/175 [08:16<01:42,  3.42s/it]\u001b[A\n",
            " 83% 146/175 [08:19<01:37,  3.36s/it]\u001b[A\n",
            " 84% 147/175 [08:23<01:35,  3.41s/it]\u001b[A\n",
            " 85% 148/175 [08:26<01:31,  3.39s/it]\u001b[A\n",
            " 85% 149/175 [08:29<01:25,  3.28s/it]\u001b[A\n",
            " 86% 150/175 [08:33<01:24,  3.39s/it]\u001b[A\n",
            " 86% 151/175 [08:36<01:21,  3.42s/it]\u001b[A\n",
            " 87% 152/175 [08:40<01:18,  3.42s/it]\u001b[A\n",
            " 87% 153/175 [08:43<01:15,  3.44s/it]\u001b[A\n",
            " 88% 154/175 [08:47<01:12,  3.44s/it]\u001b[A\n",
            " 89% 155/175 [08:50<01:08,  3.45s/it]\u001b[A\n",
            " 89% 156/175 [08:54<01:05,  3.45s/it]\u001b[A\n",
            " 90% 157/175 [08:57<01:01,  3.42s/it]\u001b[A\n",
            " 90% 158/175 [09:00<00:57,  3.40s/it]\u001b[A\n",
            " 91% 159/175 [09:04<00:54,  3.41s/it]\u001b[A\n",
            " 91% 160/175 [09:07<00:50,  3.39s/it]\u001b[A\n",
            " 92% 161/175 [09:11<00:47,  3.38s/it]\u001b[A\n",
            " 93% 162/175 [09:14<00:44,  3.43s/it]\u001b[A\n",
            " 93% 163/175 [09:17<00:39,  3.33s/it]\u001b[A\n",
            " 94% 164/175 [09:21<00:36,  3.33s/it]\u001b[A\n",
            " 94% 165/175 [09:24<00:33,  3.37s/it]\u001b[A\n",
            " 95% 166/175 [09:28<00:30,  3.42s/it]\u001b[A\n",
            " 95% 167/175 [09:31<00:26,  3.37s/it]\u001b[A\n",
            " 96% 168/175 [09:34<00:23,  3.43s/it]\u001b[A\n",
            " 97% 169/175 [09:38<00:20,  3.39s/it]\u001b[A\n",
            " 97% 170/175 [09:41<00:16,  3.38s/it]\u001b[A\n",
            " 98% 171/175 [09:44<00:13,  3.33s/it]\u001b[A\n",
            " 98% 172/175 [09:48<00:10,  3.40s/it]\u001b[A\n",
            " 99% 173/175 [09:51<00:06,  3.38s/it]\u001b[A\n",
            " 99% 174/175 [09:54<00:03,  3.34s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.3228323459625244, 'eval_runtime': 601.4472, 'eval_samples_per_second': 0.291, 'eval_steps_per_second': 0.291, 'epoch': 2.8}\n",
            " 93% 700/750 [2:16:06<34:46, 41.73s/it]\n",
            "100% 175/175 [09:58<00:00,  3.31s/it]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:4074] 2025-08-10 09:26:47,449 >> Saving model checkpoint to /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-700\n",
            "[INFO|configuration_utils.py:752] 2025-08-10 09:26:47,731 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-10 09:26:47,732 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 09:26:48,992 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-700/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 09:26:48,998 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-700/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 09:26:49,005 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-700/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 09:26:53,474 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 09:26:53,479 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 09:26:53,484 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/special_tokens_map.json\n",
            "{'loss': 0.2218, 'grad_norm': 0.3236859440803528, 'learning_rate': 9.075732372720414e-07, 'epoch': 2.84}\n",
            "{'loss': 0.2303, 'grad_norm': 0.3349764347076416, 'learning_rate': 5.19519768082738e-07, 'epoch': 2.88}\n",
            "{'loss': 0.2157, 'grad_norm': 0.3492169678211212, 'learning_rate': 2.386300009084408e-07, 'epoch': 2.92}\n",
            "{'loss': 0.2206, 'grad_norm': 0.32841795682907104, 'learning_rate': 6.551228043715219e-08, 'epoch': 2.96}\n",
            "{'loss': 0.2342, 'grad_norm': 0.32829201221466064, 'learning_rate': 5.415411020615047e-10, 'epoch': 3.0}\n",
            "100% 750/750 [2:51:34<00:00, 42.85s/it][INFO|trainer.py:4408] 2025-08-10 10:02:15,978 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4410] 2025-08-10 10:02:15,978 >>   Num examples = 175\n",
            "[INFO|trainer.py:4413] 2025-08-10 10:02:15,978 >>   Batch size = 1\n",
            "\n",
            "  0% 0/175 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/175 [00:03<04:48,  1.67s/it]\u001b[A\n",
            "  2% 3/175 [00:06<06:38,  2.32s/it]\u001b[A\n",
            "  2% 4/175 [00:10<08:02,  2.82s/it]\u001b[A\n",
            "  3% 5/175 [00:13<08:36,  3.04s/it]\u001b[A\n",
            "  3% 6/175 [00:17<08:56,  3.18s/it]\u001b[A\n",
            "  4% 7/175 [00:20<09:21,  3.34s/it]\u001b[A\n",
            "  5% 8/175 [00:24<09:29,  3.41s/it]\u001b[A\n",
            "  5% 9/175 [00:27<09:27,  3.42s/it]\u001b[A\n",
            "  6% 10/175 [00:31<09:20,  3.40s/it]\u001b[A\n",
            "  6% 11/175 [00:34<09:25,  3.45s/it]\u001b[A\n",
            "  7% 12/175 [00:38<09:39,  3.56s/it]\u001b[A\n",
            "  7% 13/175 [00:41<09:20,  3.46s/it]\u001b[A\n",
            "  8% 14/175 [00:45<09:18,  3.47s/it]\u001b[A\n",
            "  9% 15/175 [00:48<09:14,  3.47s/it]\u001b[A\n",
            "  9% 16/175 [00:52<09:26,  3.56s/it]\u001b[A\n",
            " 10% 17/175 [00:55<09:18,  3.54s/it]\u001b[A\n",
            " 10% 18/175 [00:58<08:50,  3.38s/it]\u001b[A\n",
            " 11% 19/175 [01:02<09:06,  3.50s/it]\u001b[A\n",
            " 11% 20/175 [01:06<09:17,  3.60s/it]\u001b[A\n",
            " 12% 21/175 [01:10<09:12,  3.59s/it]\u001b[A\n",
            " 13% 22/175 [01:13<08:58,  3.52s/it]\u001b[A\n",
            " 13% 23/175 [01:16<08:25,  3.33s/it]\u001b[A\n",
            " 14% 24/175 [01:19<08:19,  3.31s/it]\u001b[A\n",
            " 14% 25/175 [01:23<08:32,  3.41s/it]\u001b[A\n",
            " 15% 26/175 [01:26<08:24,  3.39s/it]\u001b[A\n",
            " 15% 27/175 [01:30<08:28,  3.44s/it]\u001b[A\n",
            " 16% 28/175 [01:33<08:26,  3.45s/it]\u001b[A\n",
            " 17% 29/175 [01:36<08:13,  3.38s/it]\u001b[A\n",
            " 17% 30/175 [01:40<08:13,  3.40s/it]\u001b[A\n",
            " 18% 31/175 [01:43<08:16,  3.45s/it]\u001b[A\n",
            " 18% 32/175 [01:47<08:12,  3.44s/it]\u001b[A\n",
            " 19% 33/175 [01:50<08:13,  3.48s/it]\u001b[A\n",
            " 19% 34/175 [01:54<08:27,  3.60s/it]\u001b[A\n",
            " 20% 35/175 [01:58<08:17,  3.55s/it]\u001b[A\n",
            " 21% 36/175 [02:01<08:10,  3.53s/it]\u001b[A\n",
            " 21% 37/175 [02:05<08:04,  3.51s/it]\u001b[A\n",
            " 22% 38/175 [02:08<07:54,  3.46s/it]\u001b[A\n",
            " 22% 39/175 [02:11<07:50,  3.46s/it]\u001b[A\n",
            " 23% 40/175 [02:15<07:50,  3.48s/it]\u001b[A\n",
            " 23% 41/175 [02:18<07:46,  3.48s/it]\u001b[A\n",
            " 24% 42/175 [02:22<07:45,  3.50s/it]\u001b[A\n",
            " 25% 43/175 [02:25<07:27,  3.39s/it]\u001b[A\n",
            " 25% 44/175 [02:29<07:30,  3.44s/it]\u001b[A\n",
            " 26% 45/175 [02:32<07:36,  3.51s/it]\u001b[A\n",
            " 26% 46/175 [02:36<07:39,  3.56s/it]\u001b[A\n",
            " 27% 47/175 [02:40<07:35,  3.56s/it]\u001b[A\n",
            " 27% 48/175 [02:43<07:31,  3.56s/it]\u001b[A\n",
            " 28% 49/175 [02:47<07:28,  3.56s/it]\u001b[A\n",
            " 29% 50/175 [02:50<07:08,  3.43s/it]\u001b[A\n",
            " 29% 51/175 [02:54<07:13,  3.50s/it]\u001b[A\n",
            " 30% 52/175 [02:57<07:12,  3.51s/it]\u001b[A\n",
            " 30% 53/175 [03:01<07:19,  3.60s/it]\u001b[A\n",
            " 31% 54/175 [03:04<07:02,  3.49s/it]\u001b[A\n",
            " 31% 55/175 [03:08<06:58,  3.48s/it]\u001b[A\n",
            " 32% 56/175 [03:11<07:01,  3.54s/it]\u001b[A\n",
            " 33% 57/175 [03:15<06:51,  3.48s/it]\u001b[A\n",
            " 33% 58/175 [03:18<06:38,  3.41s/it]\u001b[A\n",
            " 34% 59/175 [03:21<06:36,  3.42s/it]\u001b[A\n",
            " 34% 60/175 [03:24<06:22,  3.33s/it]\u001b[A\n",
            " 35% 61/175 [03:28<06:19,  3.33s/it]\u001b[A\n",
            " 35% 62/175 [03:31<06:20,  3.37s/it]\u001b[A\n",
            " 36% 63/175 [03:35<06:23,  3.43s/it]\u001b[A\n",
            " 37% 64/175 [03:38<06:20,  3.43s/it]\u001b[A\n",
            " 37% 65/175 [03:42<06:18,  3.44s/it]\u001b[A\n",
            " 38% 66/175 [03:45<06:18,  3.47s/it]\u001b[A\n",
            " 38% 67/175 [03:49<06:14,  3.47s/it]\u001b[A\n",
            " 39% 68/175 [03:52<06:10,  3.47s/it]\u001b[A\n",
            " 39% 69/175 [03:56<06:06,  3.46s/it]\u001b[A\n",
            " 40% 70/175 [03:59<06:10,  3.53s/it]\u001b[A\n",
            " 41% 71/175 [04:03<06:07,  3.54s/it]\u001b[A\n",
            " 41% 72/175 [04:06<06:08,  3.57s/it]\u001b[A\n",
            " 42% 73/175 [04:10<05:56,  3.49s/it]\u001b[A\n",
            " 42% 74/175 [04:13<05:51,  3.48s/it]\u001b[A\n",
            " 43% 75/175 [04:17<05:42,  3.43s/it]\u001b[A\n",
            " 43% 76/175 [04:20<05:39,  3.43s/it]\u001b[A\n",
            " 44% 77/175 [04:23<05:36,  3.43s/it]\u001b[A\n",
            " 45% 78/175 [04:27<05:30,  3.40s/it]\u001b[A\n",
            " 45% 79/175 [04:30<05:24,  3.38s/it]\u001b[A\n",
            " 46% 80/175 [04:33<05:19,  3.37s/it]\u001b[A\n",
            " 46% 81/175 [04:37<05:18,  3.39s/it]\u001b[A\n",
            " 47% 82/175 [04:41<05:23,  3.48s/it]\u001b[A\n",
            " 47% 83/175 [04:44<05:21,  3.50s/it]\u001b[A\n",
            " 48% 84/175 [04:48<05:23,  3.55s/it]\u001b[A\n",
            " 49% 85/175 [04:51<05:16,  3.52s/it]\u001b[A\n",
            " 49% 86/175 [04:55<05:13,  3.53s/it]\u001b[A\n",
            " 50% 87/175 [04:58<05:10,  3.53s/it]\u001b[A\n",
            " 50% 88/175 [05:02<05:01,  3.47s/it]\u001b[A\n",
            " 51% 89/175 [05:05<05:00,  3.49s/it]\u001b[A\n",
            " 51% 90/175 [05:09<04:55,  3.48s/it]\u001b[A\n",
            " 52% 91/175 [05:12<04:46,  3.41s/it]\u001b[A\n",
            " 53% 92/175 [05:15<04:43,  3.42s/it]\u001b[A\n",
            " 53% 93/175 [05:18<04:30,  3.29s/it]\u001b[A\n",
            " 54% 94/175 [05:22<04:24,  3.27s/it]\u001b[A\n",
            " 54% 95/175 [05:25<04:22,  3.29s/it]\u001b[A\n",
            " 55% 96/175 [05:28<04:20,  3.30s/it]\u001b[A\n",
            " 55% 97/175 [05:32<04:23,  3.38s/it]\u001b[A\n",
            " 56% 98/175 [05:35<04:24,  3.43s/it]\u001b[A\n",
            " 57% 99/175 [05:39<04:18,  3.41s/it]\u001b[A\n",
            " 57% 100/175 [05:42<04:16,  3.42s/it]\u001b[A\n",
            " 58% 101/175 [05:45<04:08,  3.36s/it]\u001b[A\n",
            " 58% 102/175 [05:49<04:09,  3.42s/it]\u001b[A\n",
            " 59% 103/175 [05:52<04:04,  3.39s/it]\u001b[A\n",
            " 59% 104/175 [05:56<04:09,  3.51s/it]\u001b[A\n",
            " 60% 105/175 [06:00<04:06,  3.52s/it]\u001b[A\n",
            " 61% 106/175 [06:03<04:01,  3.50s/it]\u001b[A\n",
            " 61% 107/175 [06:06<03:55,  3.46s/it]\u001b[A\n",
            " 62% 108/175 [06:10<03:51,  3.46s/it]\u001b[A\n",
            " 62% 109/175 [06:13<03:43,  3.39s/it]\u001b[A\n",
            " 63% 110/175 [06:16<03:38,  3.37s/it]\u001b[A\n",
            " 63% 111/175 [06:20<03:40,  3.45s/it]\u001b[A\n",
            " 64% 112/175 [06:23<03:37,  3.45s/it]\u001b[A\n",
            " 65% 113/175 [06:27<03:36,  3.49s/it]\u001b[A\n",
            " 65% 114/175 [06:30<03:25,  3.37s/it]\u001b[A\n",
            " 66% 115/175 [06:34<03:23,  3.39s/it]\u001b[A\n",
            " 66% 116/175 [06:37<03:25,  3.48s/it]\u001b[A\n",
            " 67% 117/175 [06:41<03:21,  3.47s/it]\u001b[A\n",
            " 67% 118/175 [06:44<03:15,  3.43s/it]\u001b[A\n",
            " 68% 119/175 [06:47<03:09,  3.38s/it]\u001b[A\n",
            " 69% 120/175 [06:51<03:06,  3.39s/it]\u001b[A\n",
            " 69% 121/175 [06:54<03:00,  3.34s/it]\u001b[A\n",
            " 70% 122/175 [06:57<02:55,  3.31s/it]\u001b[A\n",
            " 70% 123/175 [07:01<02:55,  3.38s/it]\u001b[A\n",
            " 71% 124/175 [07:04<02:51,  3.36s/it]\u001b[A\n",
            " 71% 125/175 [07:07<02:47,  3.36s/it]\u001b[A\n",
            " 72% 126/175 [07:11<02:46,  3.39s/it]\u001b[A\n",
            " 73% 127/175 [07:15<02:46,  3.47s/it]\u001b[A\n",
            " 73% 128/175 [07:18<02:39,  3.40s/it]\u001b[A\n",
            " 74% 129/175 [07:21<02:36,  3.41s/it]\u001b[A\n",
            " 74% 130/175 [07:25<02:35,  3.46s/it]\u001b[A\n",
            " 75% 131/175 [07:28<02:34,  3.52s/it]\u001b[A\n",
            " 75% 132/175 [07:32<02:31,  3.53s/it]\u001b[A\n",
            " 76% 133/175 [07:36<02:29,  3.57s/it]\u001b[A\n",
            " 77% 134/175 [07:39<02:23,  3.49s/it]\u001b[A\n",
            " 77% 135/175 [07:42<02:19,  3.48s/it]\u001b[A\n",
            " 78% 136/175 [07:46<02:12,  3.41s/it]\u001b[A\n",
            " 78% 137/175 [07:49<02:08,  3.38s/it]\u001b[A\n",
            " 79% 138/175 [07:52<02:05,  3.40s/it]\u001b[A\n",
            " 79% 139/175 [07:56<02:00,  3.34s/it]\u001b[A\n",
            " 80% 140/175 [07:59<01:57,  3.37s/it]\u001b[A\n",
            " 81% 141/175 [08:02<01:53,  3.35s/it]\u001b[A\n",
            " 81% 142/175 [08:06<01:53,  3.45s/it]\u001b[A\n",
            " 82% 143/175 [08:09<01:46,  3.32s/it]\u001b[A\n",
            " 82% 144/175 [08:13<01:45,  3.39s/it]\u001b[A\n",
            " 83% 145/175 [08:16<01:42,  3.41s/it]\u001b[A\n",
            " 83% 146/175 [08:19<01:37,  3.36s/it]\u001b[A\n",
            " 84% 147/175 [08:23<01:35,  3.41s/it]\u001b[A\n",
            " 85% 148/175 [08:26<01:31,  3.40s/it]\u001b[A\n",
            " 85% 149/175 [08:29<01:25,  3.28s/it]\u001b[A\n",
            " 86% 150/175 [08:33<01:25,  3.40s/it]\u001b[A\n",
            " 86% 151/175 [08:36<01:22,  3.43s/it]\u001b[A\n",
            " 87% 152/175 [08:40<01:18,  3.43s/it]\u001b[A\n",
            " 87% 153/175 [08:43<01:15,  3.45s/it]\u001b[A\n",
            " 88% 154/175 [08:47<01:12,  3.46s/it]\u001b[A\n",
            " 89% 155/175 [08:50<01:09,  3.46s/it]\u001b[A\n",
            " 89% 156/175 [08:54<01:05,  3.46s/it]\u001b[A\n",
            " 90% 157/175 [08:57<01:01,  3.43s/it]\u001b[A\n",
            " 90% 158/175 [09:00<00:57,  3.41s/it]\u001b[A\n",
            " 91% 159/175 [09:04<00:54,  3.42s/it]\u001b[A\n",
            " 91% 160/175 [09:07<00:50,  3.40s/it]\u001b[A\n",
            " 92% 161/175 [09:10<00:47,  3.38s/it]\u001b[A\n",
            " 93% 162/175 [09:14<00:44,  3.43s/it]\u001b[A\n",
            " 93% 163/175 [09:17<00:39,  3.33s/it]\u001b[A\n",
            " 94% 164/175 [09:20<00:36,  3.33s/it]\u001b[A\n",
            " 94% 165/175 [09:24<00:33,  3.36s/it]\u001b[A\n",
            " 95% 166/175 [09:27<00:30,  3.42s/it]\u001b[A\n",
            " 95% 167/175 [09:31<00:26,  3.36s/it]\u001b[A\n",
            " 96% 168/175 [09:34<00:23,  3.42s/it]\u001b[A\n",
            " 97% 169/175 [09:38<00:20,  3.38s/it]\u001b[A\n",
            " 97% 170/175 [09:41<00:16,  3.37s/it]\u001b[A\n",
            " 98% 171/175 [09:44<00:13,  3.33s/it]\u001b[A\n",
            " 98% 172/175 [09:48<00:10,  3.40s/it]\u001b[A\n",
            " 99% 173/175 [09:51<00:06,  3.38s/it]\u001b[A\n",
            " 99% 174/175 [09:54<00:03,  3.33s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.32247117161750793, 'eval_runtime': 601.3673, 'eval_samples_per_second': 0.291, 'eval_steps_per_second': 0.291, 'epoch': 3.0}\n",
            "100% 750/750 [3:01:35<00:00, 42.85s/it]\n",
            "100% 175/175 [09:58<00:00,  3.31s/it]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:4074] 2025-08-10 10:12:17,348 >> Saving model checkpoint to /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-750\n",
            "[INFO|configuration_utils.py:752] 2025-08-10 10:12:17,683 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-10 10:12:17,685 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 10:12:18,814 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-750/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 10:12:19,430 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 10:12:19,436 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/checkpoint-750/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 10:12:24,401 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 10:12:24,733 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 10:12:24,738 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/special_tokens_map.json\n",
            "[INFO|trainer.py:2718] 2025-08-10 10:12:25,126 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 10906.0811, 'train_samples_per_second': 0.275, 'train_steps_per_second': 0.069, 'train_loss': 0.058163558959960934, 'epoch': 3.0}\n",
            "100% 750/750 [3:01:43<00:00, 14.54s/it]\n",
            "[INFO|trainer.py:4875] 2025-08-10 10:12:25,143 >> Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n",
            "[INFO|trainer.py:4074] 2025-08-10 10:13:02,616 >> Saving model checkpoint to /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/\n",
            "[INFO|configuration_utils.py:752] 2025-08-10 10:13:02,904 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-10 10:13:02,905 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 10:13:04,582 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 10:13:04,589 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 10:13:04,594 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/special_tokens_map.json\n",
            "[INFO|trainer.py:4074] 2025-08-10 10:13:04,846 >> Saving model checkpoint to /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/\n",
            "[INFO|configuration_utils.py:752] 2025-08-10 10:13:05,098 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-10 10:13:05,100 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 10:13:07,023 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 10:13:07,615 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 10:13:07,623 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/special_tokens_map.json\n",
            "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
            "New Data Upload                         : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...en_models/adapter_model.safetensors:   3% 8.36M/295M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :   6% 19.8M/307M [00:00<00:04, 61.4MB/s,   ???B/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  15% 45.0M/307M [00:00<00:02, 93.4MB/s,  126MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  20% 61.8M/307M [00:00<00:02, 89.5MB/s,  105MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  26% 78.5M/307M [00:00<00:02, 87.5MB/s, 97.8MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  37% 112M/307M [00:01<00:01, 115MB/s,  115MB/s  ]  \n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  47% 146M/307M [00:01<00:01, 132MB/s,  126MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  58% 179M/307M [00:01<00:00, 144MB/s,  133MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  72% 221M/307M [00:01<00:00, 164MB/s,  144MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  83% 255M/307M [00:01<00:00, 165MB/s,  147MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  91% 280M/307M [00:02<00:00, 153MB/s,  144MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)                : 100% 307M/307M [00:02<00:00, 148MB/s,  144MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...en_models/adapter_model.safetensors: 100% 295M/295M [00:02<00:00, 131MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...en_models/adapter_model.safetensors: 100% 295M/295M [00:02<00:00, 125MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)                : 100% 307M/307M [00:02<00:00, 113MB/s,  120MB/s  ]\n",
            "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:02<?, ?B/s]\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:02<?, ?B/s]\n",
            "  ...en_models/adapter_model.safetensors: 100% 295M/295M [00:02<00:00, 118MB/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 49779930GF\n",
            "  train_loss               =     0.0582\n",
            "  train_runtime            = 3:01:46.08\n",
            "  train_samples_per_second =      0.275\n",
            "  train_steps_per_second   =      0.069\n",
            "Figure saved at: /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/training_loss.png\n",
            "Figure saved at: /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/training_eval_loss.png\n",
            "[WARNING|2025-08-10 10:13:16] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:4408] 2025-08-10 10:13:16,550 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4410] 2025-08-10 10:13:16,550 >>   Num examples = 175\n",
            "[INFO|trainer.py:4413] 2025-08-10 10:13:16,550 >>   Batch size = 1\n",
            "100% 175/175 [09:58<00:00,  3.42s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_loss               =     0.3225\n",
            "  eval_runtime            = 0:10:02.13\n",
            "  eval_samples_per_second =      0.291\n",
            "  eval_steps_per_second   =      0.291\n",
            "[INFO|trainer.py:4074] 2025-08-10 10:23:18,704 >> Saving model checkpoint to /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/\n",
            "[INFO|configuration_utils.py:752] 2025-08-10 10:23:18,970 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:817] 2025-08-10 10:23:18,972 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"layer_types\": [\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\",\n",
            "    \"full_attention\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2393] 2025-08-10 10:23:20,210 >> chat template saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2562] 2025-08-10 10:23:20,217 >> tokenizer config file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2571] 2025-08-10 10:23:20,222 >> Special tokens file saved in /gdrive/MyDrive/Artificial Intelligence/LLM/Fine Tunning/Qwen_models/special_tokens_map.json\n",
            "[INFO|modelcard.py:456] 2025-08-10 10:23:20,556 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
            "New Data Upload                         : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...en_models/adapter_model.safetensors:   6% 16.7M/295M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :   9% 28.1M/307M [00:00<00:02, 122MB/s,   ???B/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  20% 61.8M/307M [00:00<00:01, 148MB/s,  168MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  31% 95.3M/307M [00:00<00:01, 157MB/s,  168MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  42% 129M/307M [00:00<00:01, 161MB/s,  168MB/s  ] \n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  53% 162M/307M [00:01<00:00, 163MB/s,  168MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  61% 188M/307M [00:01<00:00, 150MB/s,  159MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  75% 229M/307M [00:01<00:00, 170MB/s,  168MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  86% 263M/307M [00:01<00:00, 169MB/s,  168MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (2 / 3)                :  94% 288M/307M [00:01<00:00, 155MB/s,  162MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:01<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:01<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)                : 100% 307M/307M [00:02<00:00, 137MB/s,  155MB/s  ]\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...en_models/adapter_model.safetensors: 100% 295M/295M [00:01<00:00, 139MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...en_models/adapter_model.safetensors: 100% 295M/295M [00:02<00:00, 127MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  ...en_models/adapter_model.safetensors: 100% 295M/295M [00:02<00:00, 127MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:02<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:02<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Processing Files (3 / 3)                : 100% 307M/307M [00:02<00:00, 117MB/s,  116MB/s  ]\n",
            "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...nning/Qwen_models/training_args.bin: 100% 5.94k/5.94k [00:02<?, ?B/s]\n",
            "  ... Tunning/Qwen_models/tokenizer.json: 100% 11.4M/11.4M [00:02<?, ?B/s]\n",
            "  ...en_models/adapter_model.safetensors: 100% 295M/295M [00:02<00:00, 116MB/s]\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mquestions_generations_Qwen\u001b[0m at: \u001b[34mhttps://wandb.ai/christeenhallak33-coretech-mena/llamafactory/runs/9bug1y1a\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250810_071039-9bug1y1a/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# STEP 4: Run training using the example config\n",
        "!llamafactory-cli train examples/train_lora/news_finetune.yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3FcPDy1sTKK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}